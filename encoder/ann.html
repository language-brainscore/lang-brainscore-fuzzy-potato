<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>langbrainscore.encoder.ann API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>langbrainscore.encoder.ann</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import typing
from enum import unique

import os
import numpy as np
import torch
from tqdm import tqdm
import xarray as xr

from langbrainscore.dataset import Dataset
from langbrainscore.interface import EncoderRepresentations, _ModelEncoder
from langbrainscore.utils.encoder import (
    aggregate_layers,
    cos_sim_matrix,
    count_zero_threshold_values,
    flatten_activations_per_sample,
    get_context_groups,
    get_torch_device,
    pick_matching_token_ixs,
    postprocess_activations,
    repackage_flattened_activations,
    encode_stimuli_in_context,
)

from langbrainscore.utils.logging import log
from langbrainscore.utils.xarray import copy_metadata, fix_xr_dtypes
from langbrainscore.utils.resources import model_classes, config_name_mappings

os.environ[&#34;TOKENIZERS_PARALLELISM&#34;] = &#34;true&#34;


class HuggingFaceEncoder(_ModelEncoder):
    def __init__(
        self,
        model_id,
        emb_aggregation: typing.Union[str, None, typing.Callable],
        device=get_torch_device(),
        context_dimension: str = None,
        bidirectional: bool = False,
        emb_preproc: typing.Tuple[str] = (),
        include_special_tokens: bool = True,
    ):
        &#34;&#34;&#34;
        Args:
            model_id (str): the model id
            device (None, ?): the device to use
            context_dimension (str, optional): the dimension to use for extracting strings using context.
                if None, each sampleid (stimuli) will be treated as a single context group.
                if a string is specified, the string must refer to the name of a dimension in the xarray-like dataset
                object (langbrainscore.dataset.Dataset) that provides groupings of sampleids (stimuli) that should be
                used as context when generating encoder representations [default: None].
            bidirectional (bool): whether to use bidirectional encoder (i.e., access both forward and backward context)
                [default: False]
            emb_aggregation (typing.Union[str, None, typing.Callable], optional): how to aggregate the hidden states of
                the encoder representations for each sampleid (stimuli). [default: &#34;last&#34;]
            emb_preproc (tuple): a list of strings specifying preprocessing functions to apply to the aggregated embeddings.
                Processing is performed layer-wise.
            include_special_tokens (bool): whether to include special tokens in the encoder representations.
        &#34;&#34;&#34;

        super().__init__(
            model_id,
            _context_dimension=context_dimension,
            _bidirectional=bidirectional,
            _emb_aggregation=emb_aggregation,
            _emb_preproc=emb_preproc,
            _include_special_tokens=include_special_tokens,
        )

        from transformers import AutoConfig, AutoModel, AutoTokenizer
        from transformers import logging as transformers_logging

        transformers_logging.set_verbosity_error()

        self.device = device or get_torch_device()
        self.config = AutoConfig.from_pretrained(self._model_id)
        self.tokenizer = AutoTokenizer.from_pretrained(
            self._model_id, multiprocessing=True
        )
        self.model = AutoModel.from_pretrained(self._model_id, config=self.config)
        try:
            self.model = self.model.to(self.device)
        except RuntimeError:
            self.device = &#34;cpu&#34;
            self.model = self.model.to(self.device)

    def get_encoder_representations_template(
        self, dataset=None, representations=xr.DataArray()
    ) -&gt; EncoderRepresentations:
        &#34;&#34;&#34;
        returns an empty `EncoderRepresentations` object with all the appropriate
        attributes but the `dataset` and `representations` missing and to be filled in
        later.
        &#34;&#34;&#34;
        return EncoderRepresentations(
            dataset=dataset,
            representations=representations,
            model_id=self._model_id,
            context_dimension=self._context_dimension,
            bidirectional=self._bidirectional,
            emb_aggregation=self._emb_aggregation,
            emb_preproc=self._emb_preproc,
            include_special_tokens=self._include_special_tokens,
        )

    def encode(
        self,
        dataset: Dataset,
        read_cache: bool = True,  # avoid recomputing if cached `EncoderRepresentations` exists, recompute if not
        write_cache: bool = True,  # dump the result of this computation to cache?
    ) -&gt; EncoderRepresentations:
        &#34;&#34;&#34;
        Input a langbrainscore Dataset, encode the stimuli according to the parameters specified in init, and return
            the an xarray DataArray of aggregated representations for each stimulus.

        Args:
            dataset (langbrainscore.dataset.DataSet): [description]
            read_cache (bool): Avoid recomputing if cached `EncoderRepresentations` exists, recompute if not
            write_cache (bool): Dump and write the result of the computed encoder representations to cache

        Raises:
            NotImplementedError: [description]
            ValueError: [description]

        Returns:
            [type]: [description]
        &#34;&#34;&#34;

        # before computing the representations from scratch, we will first see if any
        # cached representations exist already.

        if read_cache:
            to_check_in_cache: EncoderRepresentations = (
                self.get_encoder_representations_template(dataset=dataset)
            )

            try:
                to_check_in_cache.load_cache()
                return to_check_in_cache
            except FileNotFoundError:
                log(
                    f&#34;couldn&#39;t load cached reprs for {to_check_in_cache.identifier_string}; recomputing.&#34;,
                    cmap=&#34;WARN&#34;,
                    type=&#34;WARN&#34;,
                )

        self.model.eval()
        stimuli = dataset.stimuli.values

        # Initialize the context group coordinate (obtain embeddings with context)
        context_groups = get_context_groups(dataset, self._context_dimension)

        # list for storing activations for each stimulus with all layers flattened
        # list for storing layer ids ([0 0 0 0 ... 1 1 1 ...]) indicating which layer each
        # neuroid (representation dimension) came from
        flattened_activations, layer_ids = [], []

        ###############################################################################
        # ALL SAMPLES LOOP
        ###############################################################################
        _, unique_ixs = np.unique(context_groups, return_index=True)
        # Make sure context group order is preserved
        for group in tqdm(context_groups[np.sort(unique_ixs)], desc=&#34;Encoding stimuli&#34;):
            # Mask based on the context group
            mask_context = context_groups == group
            stimuli_in_context = stimuli[mask_context]

            # store model states for each stimulus in this context group
            encoded_stimuli = []

            ###############################################################################
            # CONTEXT LOOP
            ###############################################################################
            for encoded_stim in encode_stimuli_in_context(
                stimuli_in_context=stimuli_in_context,
                tokenizer=self.tokenizer,
                model=self.model,
                bidirectional=self._bidirectional,
                include_special_tokens=self._include_special_tokens,
                emb_aggregation=self._emb_aggregation,
                device=self.device,
            ):
                encoded_stimuli += [encoded_stim]
            ###############################################################################
            # END CONTEXT LOOP
            ###############################################################################

            # Flatten activations across layers and package as xarray
            flattened_activations_and_layer_ids = [
                *map(flatten_activations_per_sample, encoded_stimuli)
            ]
            for f_as, l_ids in flattened_activations_and_layer_ids:
                flattened_activations += [f_as]
                layer_ids += [l_ids]
                assert len(f_as) == len(l_ids)  # Assert all layer lists are equal

        ###############################################################################
        # END ALL SAMPLES LOOP
        ###############################################################################

        # Stack flattened activations and layer ids to obtain [n_samples, emb_din * n_layers]
        activations_2d = np.vstack(flattened_activations)
        layer_ids_1d = np.squeeze(np.unique(np.vstack(layer_ids), axis=0))

        # Post-process activations after obtaining them (or &#34;pre-process&#34; them before computing brainscore)
        if len(self._emb_preproc) &gt; 0:
            for mode in self._emb_preproc:
                activations_2d, layer_ids_1d = postprocess_activations(
                    activations_2d=activations_2d,
                    layer_ids_1d=layer_ids_1d,
                    emb_preproc_mode=mode,
                )

        assert activations_2d.shape[1] == len(layer_ids_1d)
        assert activations_2d.shape[0] == len(stimuli)

        # Package activations as xarray and reapply metadata
        encoded_dataset: xr.DataArray = repackage_flattened_activations(
            activations_2d=activations_2d,
            layer_ids_1d=layer_ids_1d,
            dataset=dataset,
        )
        encoded_dataset: xr.DataArray = copy_metadata(
            encoded_dataset,
            dataset.contents,
            &#34;sampleid&#34;,
        )

        to_return: EncoderRepresentations = self.get_encoder_representations_template()
        to_return.dataset = dataset
        to_return.representations = fix_xr_dtypes(encoded_dataset)

        if write_cache:
            to_return.to_cache(overwrite=True)

        return to_return

    def get_modelcard(self):
        &#34;&#34;&#34;
        Returns the model card of the model (model-wise, and not layer-wise)
        &#34;&#34;&#34;

        model_classes = [
            &#34;gpt&#34;,
            &#34;bert&#34;,
        ]  # continuously update based on new model classes supported

        # based on the model_id, figure out which model class it is
        model_class = [x for x in model_classes if x in self._model_id][0]
        assert model_class is not None, f&#34;model_id {self._model_id} not supported&#34;

        config_specs_of_interest = config_name_mappings[model_class]

        model_specs = {}
        for (
            k_spec,
            v_spec,
        ) in (
            config_specs_of_interest.items()
        ):  # key is the name we want to use in the model card,
            # value is the name in the config
            if v_spec is not None:
                model_specs[k_spec] = getattr(self.config, v_spec)
            else:
                model_specs[k_spec] = None

        self.model_specs = model_specs

        return model_specs


class PTEncoder(_ModelEncoder):
    def __init__(self, model_id: str) -&gt; &#34;PTEncoder&#34;:
        super().__init__(model_id)

    def encode(self, dataset: &#34;langbrainscore.dataset.Dataset&#34;) -&gt; xr.DataArray:
        # TODO
        ...


class EncoderCheck:
    &#34;&#34;&#34;
    Class for checking whether obtained embeddings from the Encoder class are correct and similar to other encoder objects.
    &#34;&#34;&#34;

    def __init__(
        self,
    ):
        pass

    def _load_cached_activations(self, encoded_ann_identifier: str):
        raise NotImplementedError

    def similiarity_metric_across_layers(
        self,
        sim_metric: str = &#34;tol&#34;,
        enc1: xr.DataArray = None,
        enc2: xr.DataArray = None,
        tol: float = 1e-8,
        threshold: float = 1e-4,
    ) -&gt; bool:
        &#34;&#34;&#34;
        Given two activations, iterate across layers and check np.allclose using different tolerance levels.

                Parameters:
            sim_metric: str
                Similarity metric to use.
            enc1: xr.DataArray
                First encoder activations.
            enc2: xr.DataArray
                Second encoder activations.
            tol: float
                Tolerance level to start at (we will iterate upwards the tolerance level). Default is 1e-8.

                        Returns:
                                bool: whether the tolerance level was met (True) or not (False)
                                bad_stim: set of stimuli indices that did not meet tolerance level `threshold` (if any)

        &#34;&#34;&#34;
        # First check is whether number of layers / shapes match
        assert enc1.shape == enc2.shape
        assert (
            enc1.sampleid.values == enc2.sampleid.values
        ).all()  # ensure that we are looking at the same stimuli
        layer_ids = enc1.layer.values
        _, unique_ixs = np.unique(layer_ids, return_index=True)
        print(f&#34;\n\nChecking similarity across layers using sim_metric: {sim_metric}&#34;)

        all_good = True
        bad_stim = set()  # store indices of stimuli that are not similar

        # Iterate across layers
        for layer_id in tqdm(layer_ids[np.sort(unique_ixs)]):
            enc1_layer = enc1.isel(neuroid=(enc1.layer == layer_id))  # .squeeze()
            enc2_layer = enc2.isel(neuroid=(enc2.layer == layer_id))  # .squeeze()

            # Check whether values match. If not, iteratively increase tolerance until values match
            if sim_metric in (&#34;tol&#34;, &#34;diff&#34;):
                abs_diff = np.abs(enc1_layer - enc2_layer)
                abs_diff_per_stim = np.max(
                    abs_diff, axis=1
                )  # Obtain the biggest difference aross neuroids (units)
                while (abs_diff_per_stim &gt; tol).all():
                    tol *= 10

            elif &#34;cos&#34; in sim_metric:
                # Check cosine distance between each row, e.g., sentence vector
                cos_sim = cos_sim_matrix(enc1_layer, enc2_layer)
                cos_dist = (
                    1 - cos_sim
                )  # 0 means identical, 1 means orthogonal, 2 means opposite
                # We still want this as close to zero as possible for similar vectors.
                cos_dist_abs = np.abs(cos_dist)
                abs_diff_per_stim = cos_dist_abs

                # Check how close the cosine distance is to 0
                while (cos_dist_abs &gt; tol).all():
                    tol *= 10
            else:
                raise NotImplementedError(f&#34;Invalid `sim_metric`: {sim_metric}&#34;)

            print(f&#34;Layer {layer_id}: Similarity at tolerance: {tol:.3e}&#34;)
            if tol &gt; threshold:
                print(f&#34;WARNING: Low tolerance level&#34;)
                all_good = False
                bad_stim.update(
                    enc1.sampleid[np.where(abs_diff_per_stim &gt; tol)[0]]
                )  # get sampleids of stimuli that are not similar

        return all_good, bad_stim</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="langbrainscore.encoder.ann.EncoderCheck"><code class="flex name class">
<span>class <span class="ident">EncoderCheck</span></span>
</code></dt>
<dd>
<div class="desc"><p>Class for checking whether obtained embeddings from the Encoder class are correct and similar to other encoder objects.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class EncoderCheck:
    &#34;&#34;&#34;
    Class for checking whether obtained embeddings from the Encoder class are correct and similar to other encoder objects.
    &#34;&#34;&#34;

    def __init__(
        self,
    ):
        pass

    def _load_cached_activations(self, encoded_ann_identifier: str):
        raise NotImplementedError

    def similiarity_metric_across_layers(
        self,
        sim_metric: str = &#34;tol&#34;,
        enc1: xr.DataArray = None,
        enc2: xr.DataArray = None,
        tol: float = 1e-8,
        threshold: float = 1e-4,
    ) -&gt; bool:
        &#34;&#34;&#34;
        Given two activations, iterate across layers and check np.allclose using different tolerance levels.

                Parameters:
            sim_metric: str
                Similarity metric to use.
            enc1: xr.DataArray
                First encoder activations.
            enc2: xr.DataArray
                Second encoder activations.
            tol: float
                Tolerance level to start at (we will iterate upwards the tolerance level). Default is 1e-8.

                        Returns:
                                bool: whether the tolerance level was met (True) or not (False)
                                bad_stim: set of stimuli indices that did not meet tolerance level `threshold` (if any)

        &#34;&#34;&#34;
        # First check is whether number of layers / shapes match
        assert enc1.shape == enc2.shape
        assert (
            enc1.sampleid.values == enc2.sampleid.values
        ).all()  # ensure that we are looking at the same stimuli
        layer_ids = enc1.layer.values
        _, unique_ixs = np.unique(layer_ids, return_index=True)
        print(f&#34;\n\nChecking similarity across layers using sim_metric: {sim_metric}&#34;)

        all_good = True
        bad_stim = set()  # store indices of stimuli that are not similar

        # Iterate across layers
        for layer_id in tqdm(layer_ids[np.sort(unique_ixs)]):
            enc1_layer = enc1.isel(neuroid=(enc1.layer == layer_id))  # .squeeze()
            enc2_layer = enc2.isel(neuroid=(enc2.layer == layer_id))  # .squeeze()

            # Check whether values match. If not, iteratively increase tolerance until values match
            if sim_metric in (&#34;tol&#34;, &#34;diff&#34;):
                abs_diff = np.abs(enc1_layer - enc2_layer)
                abs_diff_per_stim = np.max(
                    abs_diff, axis=1
                )  # Obtain the biggest difference aross neuroids (units)
                while (abs_diff_per_stim &gt; tol).all():
                    tol *= 10

            elif &#34;cos&#34; in sim_metric:
                # Check cosine distance between each row, e.g., sentence vector
                cos_sim = cos_sim_matrix(enc1_layer, enc2_layer)
                cos_dist = (
                    1 - cos_sim
                )  # 0 means identical, 1 means orthogonal, 2 means opposite
                # We still want this as close to zero as possible for similar vectors.
                cos_dist_abs = np.abs(cos_dist)
                abs_diff_per_stim = cos_dist_abs

                # Check how close the cosine distance is to 0
                while (cos_dist_abs &gt; tol).all():
                    tol *= 10
            else:
                raise NotImplementedError(f&#34;Invalid `sim_metric`: {sim_metric}&#34;)

            print(f&#34;Layer {layer_id}: Similarity at tolerance: {tol:.3e}&#34;)
            if tol &gt; threshold:
                print(f&#34;WARNING: Low tolerance level&#34;)
                all_good = False
                bad_stim.update(
                    enc1.sampleid[np.where(abs_diff_per_stim &gt; tol)[0]]
                )  # get sampleids of stimuli that are not similar

        return all_good, bad_stim</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="langbrainscore.encoder.ann.EncoderCheck.similiarity_metric_across_layers"><code class="name flex">
<span>def <span class="ident">similiarity_metric_across_layers</span></span>(<span>self, sim_metric: str = 'tol', enc1: xarray.core.dataarray.DataArray = None, enc2: xarray.core.dataarray.DataArray = None, tol: float = 1e-08, threshold: float = 0.0001) ‑> bool</span>
</code></dt>
<dd>
<div class="desc"><p>Given two activations, iterate across layers and check np.allclose using different tolerance levels.</p>
<pre><code>    Parameters:
sim_metric: str
    Similarity metric to use.
enc1: xr.DataArray
    First encoder activations.
enc2: xr.DataArray
    Second encoder activations.
tol: float
    Tolerance level to start at (we will iterate upwards the tolerance level). Default is 1e-8.

            Returns:
                    bool: whether the tolerance level was met (True) or not (False)
                    bad_stim: set of stimuli indices that did not meet tolerance level &lt;code&gt;threshold&lt;/code&gt; (if any)
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def similiarity_metric_across_layers(
    self,
    sim_metric: str = &#34;tol&#34;,
    enc1: xr.DataArray = None,
    enc2: xr.DataArray = None,
    tol: float = 1e-8,
    threshold: float = 1e-4,
) -&gt; bool:
    &#34;&#34;&#34;
    Given two activations, iterate across layers and check np.allclose using different tolerance levels.

            Parameters:
        sim_metric: str
            Similarity metric to use.
        enc1: xr.DataArray
            First encoder activations.
        enc2: xr.DataArray
            Second encoder activations.
        tol: float
            Tolerance level to start at (we will iterate upwards the tolerance level). Default is 1e-8.

                    Returns:
                            bool: whether the tolerance level was met (True) or not (False)
                            bad_stim: set of stimuli indices that did not meet tolerance level `threshold` (if any)

    &#34;&#34;&#34;
    # First check is whether number of layers / shapes match
    assert enc1.shape == enc2.shape
    assert (
        enc1.sampleid.values == enc2.sampleid.values
    ).all()  # ensure that we are looking at the same stimuli
    layer_ids = enc1.layer.values
    _, unique_ixs = np.unique(layer_ids, return_index=True)
    print(f&#34;\n\nChecking similarity across layers using sim_metric: {sim_metric}&#34;)

    all_good = True
    bad_stim = set()  # store indices of stimuli that are not similar

    # Iterate across layers
    for layer_id in tqdm(layer_ids[np.sort(unique_ixs)]):
        enc1_layer = enc1.isel(neuroid=(enc1.layer == layer_id))  # .squeeze()
        enc2_layer = enc2.isel(neuroid=(enc2.layer == layer_id))  # .squeeze()

        # Check whether values match. If not, iteratively increase tolerance until values match
        if sim_metric in (&#34;tol&#34;, &#34;diff&#34;):
            abs_diff = np.abs(enc1_layer - enc2_layer)
            abs_diff_per_stim = np.max(
                abs_diff, axis=1
            )  # Obtain the biggest difference aross neuroids (units)
            while (abs_diff_per_stim &gt; tol).all():
                tol *= 10

        elif &#34;cos&#34; in sim_metric:
            # Check cosine distance between each row, e.g., sentence vector
            cos_sim = cos_sim_matrix(enc1_layer, enc2_layer)
            cos_dist = (
                1 - cos_sim
            )  # 0 means identical, 1 means orthogonal, 2 means opposite
            # We still want this as close to zero as possible for similar vectors.
            cos_dist_abs = np.abs(cos_dist)
            abs_diff_per_stim = cos_dist_abs

            # Check how close the cosine distance is to 0
            while (cos_dist_abs &gt; tol).all():
                tol *= 10
        else:
            raise NotImplementedError(f&#34;Invalid `sim_metric`: {sim_metric}&#34;)

        print(f&#34;Layer {layer_id}: Similarity at tolerance: {tol:.3e}&#34;)
        if tol &gt; threshold:
            print(f&#34;WARNING: Low tolerance level&#34;)
            all_good = False
            bad_stim.update(
                enc1.sampleid[np.where(abs_diff_per_stim &gt; tol)[0]]
            )  # get sampleids of stimuli that are not similar

    return all_good, bad_stim</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="langbrainscore.encoder.ann.HuggingFaceEncoder"><code class="flex name class">
<span>class <span class="ident">HuggingFaceEncoder</span></span>
<span>(</span><span>model_id, emb_aggregation: Union[str, None, Callable], device=device(type='cpu'), context_dimension: str = None, bidirectional: bool = False, emb_preproc: Tuple[str] = (), include_special_tokens: bool = True)</span>
</code></dt>
<dd>
<div class="desc"><p>Interface for *Encoder classes.
Must implement an <code>encode</code> method that operates on a Dataset object.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>model_id</code></strong> :&ensp;<code>str</code></dt>
<dd>the model id</dd>
<dt>device (None, ?): the device to use</dt>
<dt><strong><code>context_dimension</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>the dimension to use for extracting strings using context.
if None, each sampleid (stimuli) will be treated as a single context group.
if a string is specified, the string must refer to the name of a dimension in the xarray-like dataset
object (langbrainscore.dataset.Dataset) that provides groupings of sampleids (stimuli) that should be
used as context when generating encoder representations [default: None].</dd>
<dt><strong><code>bidirectional</code></strong> :&ensp;<code>bool</code></dt>
<dd>whether to use bidirectional encoder (i.e., access both forward and backward context)
[default: False]</dd>
<dt><strong><code>emb_aggregation</code></strong> :&ensp;<code>typing.Union[str, None, typing.Callable]</code>, optional</dt>
<dd>how to aggregate the hidden states of
the encoder representations for each sampleid (stimuli). [default: "last"]</dd>
<dt><strong><code>emb_preproc</code></strong> :&ensp;<code>tuple</code></dt>
<dd>a list of strings specifying preprocessing functions to apply to the aggregated embeddings.
Processing is performed layer-wise.</dd>
<dt><strong><code>include_special_tokens</code></strong> :&ensp;<code>bool</code></dt>
<dd>whether to include special tokens in the encoder representations.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class HuggingFaceEncoder(_ModelEncoder):
    def __init__(
        self,
        model_id,
        emb_aggregation: typing.Union[str, None, typing.Callable],
        device=get_torch_device(),
        context_dimension: str = None,
        bidirectional: bool = False,
        emb_preproc: typing.Tuple[str] = (),
        include_special_tokens: bool = True,
    ):
        &#34;&#34;&#34;
        Args:
            model_id (str): the model id
            device (None, ?): the device to use
            context_dimension (str, optional): the dimension to use for extracting strings using context.
                if None, each sampleid (stimuli) will be treated as a single context group.
                if a string is specified, the string must refer to the name of a dimension in the xarray-like dataset
                object (langbrainscore.dataset.Dataset) that provides groupings of sampleids (stimuli) that should be
                used as context when generating encoder representations [default: None].
            bidirectional (bool): whether to use bidirectional encoder (i.e., access both forward and backward context)
                [default: False]
            emb_aggregation (typing.Union[str, None, typing.Callable], optional): how to aggregate the hidden states of
                the encoder representations for each sampleid (stimuli). [default: &#34;last&#34;]
            emb_preproc (tuple): a list of strings specifying preprocessing functions to apply to the aggregated embeddings.
                Processing is performed layer-wise.
            include_special_tokens (bool): whether to include special tokens in the encoder representations.
        &#34;&#34;&#34;

        super().__init__(
            model_id,
            _context_dimension=context_dimension,
            _bidirectional=bidirectional,
            _emb_aggregation=emb_aggregation,
            _emb_preproc=emb_preproc,
            _include_special_tokens=include_special_tokens,
        )

        from transformers import AutoConfig, AutoModel, AutoTokenizer
        from transformers import logging as transformers_logging

        transformers_logging.set_verbosity_error()

        self.device = device or get_torch_device()
        self.config = AutoConfig.from_pretrained(self._model_id)
        self.tokenizer = AutoTokenizer.from_pretrained(
            self._model_id, multiprocessing=True
        )
        self.model = AutoModel.from_pretrained(self._model_id, config=self.config)
        try:
            self.model = self.model.to(self.device)
        except RuntimeError:
            self.device = &#34;cpu&#34;
            self.model = self.model.to(self.device)

    def get_encoder_representations_template(
        self, dataset=None, representations=xr.DataArray()
    ) -&gt; EncoderRepresentations:
        &#34;&#34;&#34;
        returns an empty `EncoderRepresentations` object with all the appropriate
        attributes but the `dataset` and `representations` missing and to be filled in
        later.
        &#34;&#34;&#34;
        return EncoderRepresentations(
            dataset=dataset,
            representations=representations,
            model_id=self._model_id,
            context_dimension=self._context_dimension,
            bidirectional=self._bidirectional,
            emb_aggregation=self._emb_aggregation,
            emb_preproc=self._emb_preproc,
            include_special_tokens=self._include_special_tokens,
        )

    def encode(
        self,
        dataset: Dataset,
        read_cache: bool = True,  # avoid recomputing if cached `EncoderRepresentations` exists, recompute if not
        write_cache: bool = True,  # dump the result of this computation to cache?
    ) -&gt; EncoderRepresentations:
        &#34;&#34;&#34;
        Input a langbrainscore Dataset, encode the stimuli according to the parameters specified in init, and return
            the an xarray DataArray of aggregated representations for each stimulus.

        Args:
            dataset (langbrainscore.dataset.DataSet): [description]
            read_cache (bool): Avoid recomputing if cached `EncoderRepresentations` exists, recompute if not
            write_cache (bool): Dump and write the result of the computed encoder representations to cache

        Raises:
            NotImplementedError: [description]
            ValueError: [description]

        Returns:
            [type]: [description]
        &#34;&#34;&#34;

        # before computing the representations from scratch, we will first see if any
        # cached representations exist already.

        if read_cache:
            to_check_in_cache: EncoderRepresentations = (
                self.get_encoder_representations_template(dataset=dataset)
            )

            try:
                to_check_in_cache.load_cache()
                return to_check_in_cache
            except FileNotFoundError:
                log(
                    f&#34;couldn&#39;t load cached reprs for {to_check_in_cache.identifier_string}; recomputing.&#34;,
                    cmap=&#34;WARN&#34;,
                    type=&#34;WARN&#34;,
                )

        self.model.eval()
        stimuli = dataset.stimuli.values

        # Initialize the context group coordinate (obtain embeddings with context)
        context_groups = get_context_groups(dataset, self._context_dimension)

        # list for storing activations for each stimulus with all layers flattened
        # list for storing layer ids ([0 0 0 0 ... 1 1 1 ...]) indicating which layer each
        # neuroid (representation dimension) came from
        flattened_activations, layer_ids = [], []

        ###############################################################################
        # ALL SAMPLES LOOP
        ###############################################################################
        _, unique_ixs = np.unique(context_groups, return_index=True)
        # Make sure context group order is preserved
        for group in tqdm(context_groups[np.sort(unique_ixs)], desc=&#34;Encoding stimuli&#34;):
            # Mask based on the context group
            mask_context = context_groups == group
            stimuli_in_context = stimuli[mask_context]

            # store model states for each stimulus in this context group
            encoded_stimuli = []

            ###############################################################################
            # CONTEXT LOOP
            ###############################################################################
            for encoded_stim in encode_stimuli_in_context(
                stimuli_in_context=stimuli_in_context,
                tokenizer=self.tokenizer,
                model=self.model,
                bidirectional=self._bidirectional,
                include_special_tokens=self._include_special_tokens,
                emb_aggregation=self._emb_aggregation,
                device=self.device,
            ):
                encoded_stimuli += [encoded_stim]
            ###############################################################################
            # END CONTEXT LOOP
            ###############################################################################

            # Flatten activations across layers and package as xarray
            flattened_activations_and_layer_ids = [
                *map(flatten_activations_per_sample, encoded_stimuli)
            ]
            for f_as, l_ids in flattened_activations_and_layer_ids:
                flattened_activations += [f_as]
                layer_ids += [l_ids]
                assert len(f_as) == len(l_ids)  # Assert all layer lists are equal

        ###############################################################################
        # END ALL SAMPLES LOOP
        ###############################################################################

        # Stack flattened activations and layer ids to obtain [n_samples, emb_din * n_layers]
        activations_2d = np.vstack(flattened_activations)
        layer_ids_1d = np.squeeze(np.unique(np.vstack(layer_ids), axis=0))

        # Post-process activations after obtaining them (or &#34;pre-process&#34; them before computing brainscore)
        if len(self._emb_preproc) &gt; 0:
            for mode in self._emb_preproc:
                activations_2d, layer_ids_1d = postprocess_activations(
                    activations_2d=activations_2d,
                    layer_ids_1d=layer_ids_1d,
                    emb_preproc_mode=mode,
                )

        assert activations_2d.shape[1] == len(layer_ids_1d)
        assert activations_2d.shape[0] == len(stimuli)

        # Package activations as xarray and reapply metadata
        encoded_dataset: xr.DataArray = repackage_flattened_activations(
            activations_2d=activations_2d,
            layer_ids_1d=layer_ids_1d,
            dataset=dataset,
        )
        encoded_dataset: xr.DataArray = copy_metadata(
            encoded_dataset,
            dataset.contents,
            &#34;sampleid&#34;,
        )

        to_return: EncoderRepresentations = self.get_encoder_representations_template()
        to_return.dataset = dataset
        to_return.representations = fix_xr_dtypes(encoded_dataset)

        if write_cache:
            to_return.to_cache(overwrite=True)

        return to_return

    def get_modelcard(self):
        &#34;&#34;&#34;
        Returns the model card of the model (model-wise, and not layer-wise)
        &#34;&#34;&#34;

        model_classes = [
            &#34;gpt&#34;,
            &#34;bert&#34;,
        ]  # continuously update based on new model classes supported

        # based on the model_id, figure out which model class it is
        model_class = [x for x in model_classes if x in self._model_id][0]
        assert model_class is not None, f&#34;model_id {self._model_id} not supported&#34;

        config_specs_of_interest = config_name_mappings[model_class]

        model_specs = {}
        for (
            k_spec,
            v_spec,
        ) in (
            config_specs_of_interest.items()
        ):  # key is the name we want to use in the model card,
            # value is the name in the config
            if v_spec is not None:
                model_specs[k_spec] = getattr(self.config, v_spec)
            else:
                model_specs[k_spec] = None

        self.model_specs = model_specs

        return model_specs</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>langbrainscore.interface.encoder._ModelEncoder</li>
<li>langbrainscore.interface.encoder._Encoder</li>
<li>langbrainscore.interface.cacheable._Cacheable</li>
<li>typing.Protocol</li>
<li>typing.Generic</li>
<li>abc.ABC</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="langbrainscore.encoder.ann.HuggingFaceEncoder.encode"><code class="name flex">
<span>def <span class="ident">encode</span></span>(<span>self, dataset: <a title="langbrainscore.dataset.dataset.Dataset" href="../dataset/dataset.html#langbrainscore.dataset.dataset.Dataset">Dataset</a>, read_cache: bool = True, write_cache: bool = True) ‑> <a title="langbrainscore.interface.encoder.EncoderRepresentations" href="../interface/encoder.html#langbrainscore.interface.encoder.EncoderRepresentations">EncoderRepresentations</a></span>
</code></dt>
<dd>
<div class="desc"><p>Input a langbrainscore Dataset, encode the stimuli according to the parameters specified in init, and return
the an xarray DataArray of aggregated representations for each stimulus.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dataset</code></strong> :&ensp;<code>langbrainscore.dataset.DataSet</code></dt>
<dd>[description]</dd>
<dt><strong><code>read_cache</code></strong> :&ensp;<code>bool</code></dt>
<dd>Avoid recomputing if cached <code>EncoderRepresentations</code> exists, recompute if not</dd>
<dt><strong><code>write_cache</code></strong> :&ensp;<code>bool</code></dt>
<dd>Dump and write the result of the computed encoder representations to cache</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>NotImplementedError</code></dt>
<dd>[description]</dd>
<dt><code>ValueError</code></dt>
<dd>[description]</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>[type]</code></dt>
<dd>[description]</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def encode(
    self,
    dataset: Dataset,
    read_cache: bool = True,  # avoid recomputing if cached `EncoderRepresentations` exists, recompute if not
    write_cache: bool = True,  # dump the result of this computation to cache?
) -&gt; EncoderRepresentations:
    &#34;&#34;&#34;
    Input a langbrainscore Dataset, encode the stimuli according to the parameters specified in init, and return
        the an xarray DataArray of aggregated representations for each stimulus.

    Args:
        dataset (langbrainscore.dataset.DataSet): [description]
        read_cache (bool): Avoid recomputing if cached `EncoderRepresentations` exists, recompute if not
        write_cache (bool): Dump and write the result of the computed encoder representations to cache

    Raises:
        NotImplementedError: [description]
        ValueError: [description]

    Returns:
        [type]: [description]
    &#34;&#34;&#34;

    # before computing the representations from scratch, we will first see if any
    # cached representations exist already.

    if read_cache:
        to_check_in_cache: EncoderRepresentations = (
            self.get_encoder_representations_template(dataset=dataset)
        )

        try:
            to_check_in_cache.load_cache()
            return to_check_in_cache
        except FileNotFoundError:
            log(
                f&#34;couldn&#39;t load cached reprs for {to_check_in_cache.identifier_string}; recomputing.&#34;,
                cmap=&#34;WARN&#34;,
                type=&#34;WARN&#34;,
            )

    self.model.eval()
    stimuli = dataset.stimuli.values

    # Initialize the context group coordinate (obtain embeddings with context)
    context_groups = get_context_groups(dataset, self._context_dimension)

    # list for storing activations for each stimulus with all layers flattened
    # list for storing layer ids ([0 0 0 0 ... 1 1 1 ...]) indicating which layer each
    # neuroid (representation dimension) came from
    flattened_activations, layer_ids = [], []

    ###############################################################################
    # ALL SAMPLES LOOP
    ###############################################################################
    _, unique_ixs = np.unique(context_groups, return_index=True)
    # Make sure context group order is preserved
    for group in tqdm(context_groups[np.sort(unique_ixs)], desc=&#34;Encoding stimuli&#34;):
        # Mask based on the context group
        mask_context = context_groups == group
        stimuli_in_context = stimuli[mask_context]

        # store model states for each stimulus in this context group
        encoded_stimuli = []

        ###############################################################################
        # CONTEXT LOOP
        ###############################################################################
        for encoded_stim in encode_stimuli_in_context(
            stimuli_in_context=stimuli_in_context,
            tokenizer=self.tokenizer,
            model=self.model,
            bidirectional=self._bidirectional,
            include_special_tokens=self._include_special_tokens,
            emb_aggregation=self._emb_aggregation,
            device=self.device,
        ):
            encoded_stimuli += [encoded_stim]
        ###############################################################################
        # END CONTEXT LOOP
        ###############################################################################

        # Flatten activations across layers and package as xarray
        flattened_activations_and_layer_ids = [
            *map(flatten_activations_per_sample, encoded_stimuli)
        ]
        for f_as, l_ids in flattened_activations_and_layer_ids:
            flattened_activations += [f_as]
            layer_ids += [l_ids]
            assert len(f_as) == len(l_ids)  # Assert all layer lists are equal

    ###############################################################################
    # END ALL SAMPLES LOOP
    ###############################################################################

    # Stack flattened activations and layer ids to obtain [n_samples, emb_din * n_layers]
    activations_2d = np.vstack(flattened_activations)
    layer_ids_1d = np.squeeze(np.unique(np.vstack(layer_ids), axis=0))

    # Post-process activations after obtaining them (or &#34;pre-process&#34; them before computing brainscore)
    if len(self._emb_preproc) &gt; 0:
        for mode in self._emb_preproc:
            activations_2d, layer_ids_1d = postprocess_activations(
                activations_2d=activations_2d,
                layer_ids_1d=layer_ids_1d,
                emb_preproc_mode=mode,
            )

    assert activations_2d.shape[1] == len(layer_ids_1d)
    assert activations_2d.shape[0] == len(stimuli)

    # Package activations as xarray and reapply metadata
    encoded_dataset: xr.DataArray = repackage_flattened_activations(
        activations_2d=activations_2d,
        layer_ids_1d=layer_ids_1d,
        dataset=dataset,
    )
    encoded_dataset: xr.DataArray = copy_metadata(
        encoded_dataset,
        dataset.contents,
        &#34;sampleid&#34;,
    )

    to_return: EncoderRepresentations = self.get_encoder_representations_template()
    to_return.dataset = dataset
    to_return.representations = fix_xr_dtypes(encoded_dataset)

    if write_cache:
        to_return.to_cache(overwrite=True)

    return to_return</code></pre>
</details>
</dd>
<dt id="langbrainscore.encoder.ann.HuggingFaceEncoder.get_encoder_representations_template"><code class="name flex">
<span>def <span class="ident">get_encoder_representations_template</span></span>(<span>self, dataset=None, representations=&lt;xarray.DataArray ()&gt;
array(nan)) ‑> <a title="langbrainscore.interface.encoder.EncoderRepresentations" href="../interface/encoder.html#langbrainscore.interface.encoder.EncoderRepresentations">EncoderRepresentations</a></span>
</code></dt>
<dd>
<div class="desc"><p>returns an empty <code>EncoderRepresentations</code> object with all the appropriate
attributes but the <code>dataset</code> and <code>representations</code> missing and to be filled in
later.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_encoder_representations_template(
    self, dataset=None, representations=xr.DataArray()
) -&gt; EncoderRepresentations:
    &#34;&#34;&#34;
    returns an empty `EncoderRepresentations` object with all the appropriate
    attributes but the `dataset` and `representations` missing and to be filled in
    later.
    &#34;&#34;&#34;
    return EncoderRepresentations(
        dataset=dataset,
        representations=representations,
        model_id=self._model_id,
        context_dimension=self._context_dimension,
        bidirectional=self._bidirectional,
        emb_aggregation=self._emb_aggregation,
        emb_preproc=self._emb_preproc,
        include_special_tokens=self._include_special_tokens,
    )</code></pre>
</details>
</dd>
<dt id="langbrainscore.encoder.ann.HuggingFaceEncoder.get_modelcard"><code class="name flex">
<span>def <span class="ident">get_modelcard</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the model card of the model (model-wise, and not layer-wise)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_modelcard(self):
    &#34;&#34;&#34;
    Returns the model card of the model (model-wise, and not layer-wise)
    &#34;&#34;&#34;

    model_classes = [
        &#34;gpt&#34;,
        &#34;bert&#34;,
    ]  # continuously update based on new model classes supported

    # based on the model_id, figure out which model class it is
    model_class = [x for x in model_classes if x in self._model_id][0]
    assert model_class is not None, f&#34;model_id {self._model_id} not supported&#34;

    config_specs_of_interest = config_name_mappings[model_class]

    model_specs = {}
    for (
        k_spec,
        v_spec,
    ) in (
        config_specs_of_interest.items()
    ):  # key is the name we want to use in the model card,
        # value is the name in the config
        if v_spec is not None:
            model_specs[k_spec] = getattr(self.config, v_spec)
        else:
            model_specs[k_spec] = None

    self.model_specs = model_specs

    return model_specs</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="langbrainscore.encoder.ann.PTEncoder"><code class="flex name class">
<span>class <span class="ident">PTEncoder</span></span>
<span>(</span><span>model_id: str)</span>
</code></dt>
<dd>
<div class="desc"><p>Interface for *Encoder classes.
Must implement an <code>encode</code> method that operates on a Dataset object.</p>
<p>This class is intended to be an interface for all ANN subclasses,
including HuggingFaceEncoder, and, in the future, other kinds of
ANN encoders</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>model_id</code></strong> :&ensp;<code>str</code></dt>
<dd><em>description</em></dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>_ModelEncoder</code></dt>
<dd><em>description</em></dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class PTEncoder(_ModelEncoder):
    def __init__(self, model_id: str) -&gt; &#34;PTEncoder&#34;:
        super().__init__(model_id)

    def encode(self, dataset: &#34;langbrainscore.dataset.Dataset&#34;) -&gt; xr.DataArray:
        # TODO
        ...</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>langbrainscore.interface.encoder._ModelEncoder</li>
<li>langbrainscore.interface.encoder._Encoder</li>
<li>langbrainscore.interface.cacheable._Cacheable</li>
<li>typing.Protocol</li>
<li>typing.Generic</li>
<li>abc.ABC</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="langbrainscore.encoder.ann.PTEncoder.encode"><code class="name flex">
<span>def <span class="ident">encode</span></span>(<span>self, dataset: langbrainscore.dataset.Dataset) ‑> xarray.core.dataarray.DataArray</span>
</code></dt>
<dd>
<div class="desc"><p>returns computed representations for stimuli passed in as a <code>Dataset</code> object</p>
<h2 id="args">Args</h2>
<p>langbrainscore.dataset.Dataset: a Dataset object with a member <code>xarray.DataArray</code>
instance (<code>Dataset._xr_obj</code>) containing stimuli</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>xr.DataArray</code></dt>
<dd>Model representations of each stimulus in brain dataset</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def encode(self, dataset: &#34;langbrainscore.dataset.Dataset&#34;) -&gt; xr.DataArray:
    # TODO
    ...</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="langbrainscore.encoder" href="index.html">langbrainscore.encoder</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="langbrainscore.encoder.ann.EncoderCheck" href="#langbrainscore.encoder.ann.EncoderCheck">EncoderCheck</a></code></h4>
<ul class="">
<li><code><a title="langbrainscore.encoder.ann.EncoderCheck.similiarity_metric_across_layers" href="#langbrainscore.encoder.ann.EncoderCheck.similiarity_metric_across_layers">similiarity_metric_across_layers</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="langbrainscore.encoder.ann.HuggingFaceEncoder" href="#langbrainscore.encoder.ann.HuggingFaceEncoder">HuggingFaceEncoder</a></code></h4>
<ul class="">
<li><code><a title="langbrainscore.encoder.ann.HuggingFaceEncoder.encode" href="#langbrainscore.encoder.ann.HuggingFaceEncoder.encode">encode</a></code></li>
<li><code><a title="langbrainscore.encoder.ann.HuggingFaceEncoder.get_encoder_representations_template" href="#langbrainscore.encoder.ann.HuggingFaceEncoder.get_encoder_representations_template">get_encoder_representations_template</a></code></li>
<li><code><a title="langbrainscore.encoder.ann.HuggingFaceEncoder.get_modelcard" href="#langbrainscore.encoder.ann.HuggingFaceEncoder.get_modelcard">get_modelcard</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="langbrainscore.encoder.ann.PTEncoder" href="#langbrainscore.encoder.ann.PTEncoder">PTEncoder</a></code></h4>
<ul class="">
<li><code><a title="langbrainscore.encoder.ann.PTEncoder.encode" href="#langbrainscore.encoder.ann.PTEncoder.encode">encode</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>