<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>langbrainscore.mapping.mapping API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>langbrainscore.mapping.mapping</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">from random import sample
import typing

import numpy as np
import xarray as xr
from sklearn.linear_model import Ridge, LinearRegression, LogisticRegression, RidgeCV
from langbrainscore.utils import logging

from functools import partial

# TODO: verify behavior of LeavePOut and alternatives LeavePGroupsOut, etc.
from sklearn.model_selection import (
    KFold, # KFold without regard to any balancing coord (strat_coord) or grouping coord (split_coord)
    StratifiedKFold, # KFold balancing strat_coord across train/test splits 
    GroupKFold, # KFold keeping grouping coord (split_coord) entirely in one of train/test splits (no leakage)
    StratifiedGroupKFold, # KFold doing the group thing but also the strat thing on different coords 
)
#KFold, StratifiedShuffleSplit, LeavePOut




class Mapping:
    model = None

    def __init__(self,
                 X: xr.DataArray, Y: xr.DataArray,

                 mapping_class: typing.Union[str, typing.Any] = None,
                 random_seed: int = 42, 

                 k_fold: int = 5,
                 strat_coord: str = None,

                 num_split_groups_out: int = None, # (p, the # of groups in the test split)
                 split_coord: str = None, # (grouping coord)

                 #TODO
                 # handle predict held-out subject # but then we have to do mean over ROIs
                 # because individual neuroids do not correspond
                 # we kind of already have this along the `sampleid` coordinate, but we
                 # need to implement this in the neuroid coordinate

                 **kwargs) -&gt; None:
        &#34;&#34;&#34;Initializes a Mapping object that establishes a mapping between two encoder representations.
           The mapping is initialized with certain parameters baked in, accepted as arguments to
           the init function, listed below.

        Args:
            mapping_class (typing.Union[str, typing.Any], required): [description]. 
                This Class will be instatiated to get a mapping model. E.g. LinearRegression, Ridge,
                from the sklearn family. Must implement &lt;?classifier&gt; interface
            random_seed (int, optional): [description]. Defaults to 42.
            k_fold (int, optional): [description]. Defaults to 5.
            strat_coord (str, optional): [description]. Defaults to None.
            num_split_groups_out (int, optional): [description]. Defaults to None.
            split_coord (str, optional): [description]. Defaults to None.
        &#34;&#34;&#34;
        self.random_seed = random_seed
        mapping_classes = {
            &#39;ridge&#39;: (Ridge, {&#39;alpha&#39;: 1.0}),
            &#39;ridge_cv&#39;: (RidgeCV, {&#39;alphas&#39;: np.logspace(-3, 3, 13), &#39;alpha_per_target&#39;: True}),
            &#39;linear&#39;: (LinearRegression, {}),
            None: None,
        }

        self.k_fold = k_fold or 1
        self.strat_coord = strat_coord

        self.num_split_groups_out = num_split_groups_out
        self.split_coord = split_coord

        self.mapping_class = mapping_class

        if strat_coord:
            try:
                assert (X[strat_coord].values == Y[strat_coord].values).all()
            except AssertionError as e:
                raise ValueError(f&#39;{strat_coord} coordinate does not align across X and Y&#39;)
        if split_coord:
            try:
                assert (X[split_coord].values == Y[split_coord].values).all() 
            except AssertionError as e:
                raise ValueError(f&#39;{split_coord} coordinate does not align across X and Y&#39;)
        
        # TODO:
        # make sure there are no stimuli that have NaNs in all places along the neuroid dimension

        self.X, self.Y = X, Y
        
        
        
        self.X_nan_mask = X.isnull()
        self.Y_nan_mask = Y.isnull()
        self.X_nan_removed = X.dropna(&#39;neuroid&#39;)
        self.Y_nan_removed = Y.dropna(&#39;neuroid&#39;)

        logging.log(f&#39;X shape: {X.data.shape}, NaNs: {self.X_nan_mask.sum()}; after NaN removal: {self.X_nan_removed.data.shape}&#39;)
        logging.log(f&#39;Y shape: {Y.data.shape}, NaNs: {self.Y_nan_mask.sum()}; after NaN removal: {self.Y_nan_removed.data.shape}&#39;)

        if type(mapping_class) == str:
            mapping_class, _kwargs = mapping_classes[mapping_class]
            kwargs.update(_kwargs)
        
        # to save (this model uses the entire data rather than constructing splits)
        self.full_model = mapping_class(**kwargs)
        # placeholder model with the right params that we&#39;ll reuse across splits
        self.model = mapping_class(**kwargs)
        
        logging.log(f&#39;initialized Mapping with {mapping_class}, {type(self.model)}!&#39;)


    @staticmethod
    def _extract_dense(A = None):
        &#39;&#39;&#39;
        returns a list of several xarrays each of which is dense (has no NaNs). 
        each will have a subset of the sampleids

        Args:
            A (xr.DataArray): 
        &#39;&#39;&#39;

    def extract_dense(self):
        dense_X = self._extract_dense_arrays(self.X)


    @staticmethod
    def _construct_splits(xr_dataset: xr.Dataset, # Y: xr.Dataset, 
                          strat_coord: str = None, k_folds: int = 5,
                          split_coord: str = None, num_split_groups_out: int = None,
                          random_seed: int = 42
                         ):

        sampleid = xr_dataset.sampleid.values

        if strat_coord and split_coord:
            kf = StratifiedGroupKFold(n_splits=k_folds, shuffle=True, random_state=random_seed)
            split = partial(kf.split, sampleid, y=xr_dataset[split_coord].values, groups=xr_dataset[strat_coord].values)
        elif split_coord:
            kf = GroupKFold(n_splits=k_folds)
            split = partial(kf.split, sampleid, groups=xr_dataset[split_coord].values)
        elif strat_coord:
            kf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=random_seed)
            split = partial(kf.split, sampleid, y=xr_dataset[strat_coord].values)
        else:
            kf = KFold(n_splits=k_folds, shuffle=True, random_state=random_seed)
            split = partial(kf.split, sampleid)

        logging.log(f&#39;running {type(kf)}!&#39;)
        return split()


    def construct_splits(self, A):
        return self._construct_splits(A,
                                      self.strat_coord, self.k_fold, 
                                      self.split_coord, self.num_split_groups_out,
                                      random_seed=self.random_seed)

        
    def fit_full(self, X, Y):
        # TODO
        self.fit(X, Y, k_folds=1)
        raise NotImplemented

    def fit(self, 
            #X: xr.Dataset, Y: xr.Dataset
           ) -&gt; None:
        &#34;&#34;&#34;creates a mapping model using k-fold cross-validation
            -&gt; uses params from the class initialization, uses strat_coord
               and split_coord to stratify and split across group boundaries

        Returns:
            [type]: [description]
        &#34;&#34;&#34;        

        for neuroid in self.Y.neuroid.values:

            # limit data to current neuroid, and then drop the samples that are missing data for this neuroid
            Y_slice = self.Y.sel(neuroid=neuroid).dropna(dim=&#39;sampleid&#39;)
            Y_filtered_sampleids = Y_slice.sampleid
            assert set(Y_filtered_sampleids.values).issubset(set(self.X.sampleid.values))
            logging.log(f&#39;for neuroid {neuroid}, we used {(num_retained := len(Y_filtered_sampleids))} samples; dropped {len(self.Y.sampleid) - num_retained}&#39;) 

            X_slice = self.X.sel(sampleid=Y_filtered_sampleids.values)

            # these collections store each split for our records later
            alpha_across_splits = [] # only used in case of ridge_cv # TODO
            # TODO we aren&#39;t saving this to the object instance yet
            train_indices = []
            test_indices = []

            splits = self.construct_splits(Y_slice)

            # X_test_collection = []
            Y_test_collection = []
            Y_pred_collection = []

            for train_index, test_index in splits:
                
                train_indices.append(train_index)
                test_indices.append(test_index)

                # !! NOTE the _nan_removed variants instead of X and Y
                X_train, X_test = (
                    X_slice.sel(sampleid=Y_slice.sampleid.values[train_index]),
                    X_slice.sel(sampleid=Y_slice.sampleid.values[test_index]),
                )
                y_train, y_test = (
                    Y_slice.sel(sampleid=Y_slice.sampleid.values[train_index]),
                    Y_slice.sel(sampleid=Y_slice.sampleid.values[test_index]),
                )

                y_pred_over_time = []
                for timeid in y_train.timeid:

                    # TODO: change this code for models that also have a non-singleton timeid
                    # i.e., output evolves in time (RNN?)
                    self.model.fit(X_train.sel(timeid=0), y_train.sel(timeid=timeid).values.reshape(-1, 1))

                    # deepcopy
                    y_pred = y_test.sel(timeid=timeid).copy(deep=True).expand_dims(&#39;timeid&#39;, 1)
                    y_pred.assign_coords(timeid=(&#39;timeid&#39;, [timeid]))
                    y_pred.data = self.model.predict(X_test.sel(timeid=0)) #y_pred
                    y_pred_over_time.append(y_pred)

                y_pred_over_time = xr.concat(y_pred_over_time, dim=&#39;timeid&#39;)
                Y_pred_collection.append(y_pred_over_time)

                Y_test_collection.append(y_test)

            # ACTUALLY TODO the below is no longer true:
            #   now the Y_test_collection members are xarrays with a timeid dimension/coord
            #   but Y_pred_collection is a list of numpy arrays per discrete timeid

            # the return value is a dictionary of test/pred;
            # each of test/pred is a list of lists with two levels of
            # nesting as below:
            #   first level: CV folds
            #       second level: timeids
            yield dict(test=Y_test_collection, 
                       pred=Y_pred_collection)

    
    # def map(self, source, target) -&gt; None:
    #     &#39;&#39;&#39;
    #     the works: constructs splits, fits models for each split, then evaluates the fit 
    #             of each split and returns the result (also for each split)
    #     &#39;&#39;&#39;
    #     pass

        
    def save_model(self) -&gt; None:
        &#39;&#39;&#39;TODO: stuff that needs to be saved eventually

        - model weights
        - CV stuff (if using CV); but all arguments needed for initializing, in general
            - n_splits
            - random_state
            - split indices (based on random seed)
            - params per split (alpha, lambda)
            - validation score, etc. for each CV split?
        &#39;&#39;&#39;
        pass

    def predict(self, source) -&gt; None:
        pass


class IdentityMap(Mapping):
    &#34;&#34;&#34;Identity mapping for running RSA-type analyses that don&#39;t need splits into cv folds and don&#39;t need affine maps&#34;&#34;&#34;
    
    def fit(self,
            # X: xr.Dataset, Y: xr.Dataset
            ) -&gt; None:
        &#34;&#34;&#34;creates a mapping model using k-fold cross-validation
            depending on the class initialization, uses strat_coord
            and split_coord to stratify and split across group boundaries

        Args:, groups=None, k_folds: int = 5
            X ([type]): [description]
            Y ([type]): [description]
            k_folds (int, optional): [description]. Defaults to 5.

        Returns:
            [type]: [description]
        &#34;&#34;&#34;
        
        return dict(test=[self.Y.data],
                    pred=[[self.X.data.sel(timeid=i) for i in self.X.timeid.values]])
    
    </code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="langbrainscore.mapping.mapping.IdentityMap"><code class="flex name class">
<span>class <span class="ident">IdentityMap</span></span>
<span>(</span><span>X: xarray.core.dataarray.DataArray, Y: xarray.core.dataarray.DataArray, mapping_class: Union[str, Any] = None, random_seed: int = 42, k_fold: int = 5, strat_coord: str = None, num_split_groups_out: int = None, split_coord: str = None, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Identity mapping for running RSA-type analyses that don't need splits into cv folds and don't need affine maps</p>
<p>Initializes a Mapping object that establishes a mapping between two encoder representations.
The mapping is initialized with certain parameters baked in, accepted as arguments to
the init function, listed below.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>mapping_class</code></strong> :&ensp;<code>typing.Union[str, typing.Any], required</code></dt>
<dd>[description].
This Class will be instatiated to get a mapping model. E.g. LinearRegression, Ridge,
from the sklearn family. Must implement &lt;?classifier&gt; interface</dd>
<dt><strong><code>random_seed</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>[description]. Defaults to 42.</dd>
<dt><strong><code>k_fold</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>[description]. Defaults to 5.</dd>
<dt><strong><code>strat_coord</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>[description]. Defaults to None.</dd>
<dt><strong><code>num_split_groups_out</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>[description]. Defaults to None.</dd>
<dt><strong><code>split_coord</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>[description]. Defaults to None.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class IdentityMap(Mapping):
    &#34;&#34;&#34;Identity mapping for running RSA-type analyses that don&#39;t need splits into cv folds and don&#39;t need affine maps&#34;&#34;&#34;
    
    def fit(self,
            # X: xr.Dataset, Y: xr.Dataset
            ) -&gt; None:
        &#34;&#34;&#34;creates a mapping model using k-fold cross-validation
            depending on the class initialization, uses strat_coord
            and split_coord to stratify and split across group boundaries

        Args:, groups=None, k_folds: int = 5
            X ([type]): [description]
            Y ([type]): [description]
            k_folds (int, optional): [description]. Defaults to 5.

        Returns:
            [type]: [description]
        &#34;&#34;&#34;
        
        return dict(test=[self.Y.data],
                    pred=[[self.X.data.sel(timeid=i) for i in self.X.timeid.values]])</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="langbrainscore.mapping.mapping.Mapping" href="#langbrainscore.mapping.mapping.Mapping">Mapping</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="langbrainscore.mapping.mapping.IdentityMap.fit"><code class="name flex">
<span>def <span class="ident">fit</span></span>(<span>self) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>creates a mapping model using k-fold cross-validation
depending on the class initialization, uses strat_coord
and split_coord to stratify and split across group boundaries</p>
<p>Args:, groups=None, k_folds: int = 5
X ([type]): [description]
Y ([type]): [description]
k_folds (int, optional): [description]. Defaults to 5.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>[type]</code></dt>
<dd>[description]</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fit(self,
        # X: xr.Dataset, Y: xr.Dataset
        ) -&gt; None:
    &#34;&#34;&#34;creates a mapping model using k-fold cross-validation
        depending on the class initialization, uses strat_coord
        and split_coord to stratify and split across group boundaries

    Args:, groups=None, k_folds: int = 5
        X ([type]): [description]
        Y ([type]): [description]
        k_folds (int, optional): [description]. Defaults to 5.

    Returns:
        [type]: [description]
    &#34;&#34;&#34;
    
    return dict(test=[self.Y.data],
                pred=[[self.X.data.sel(timeid=i) for i in self.X.timeid.values]])</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="langbrainscore.mapping.mapping.Mapping" href="#langbrainscore.mapping.mapping.Mapping">Mapping</a></b></code>:
<ul class="hlist">
<li><code><a title="langbrainscore.mapping.mapping.Mapping.save_model" href="#langbrainscore.mapping.mapping.Mapping.save_model">save_model</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="langbrainscore.mapping.mapping.Mapping"><code class="flex name class">
<span>class <span class="ident">Mapping</span></span>
<span>(</span><span>X: xarray.core.dataarray.DataArray, Y: xarray.core.dataarray.DataArray, mapping_class: Union[str, Any] = None, random_seed: int = 42, k_fold: int = 5, strat_coord: str = None, num_split_groups_out: int = None, split_coord: str = None, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Initializes a Mapping object that establishes a mapping between two encoder representations.
The mapping is initialized with certain parameters baked in, accepted as arguments to
the init function, listed below.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>mapping_class</code></strong> :&ensp;<code>typing.Union[str, typing.Any], required</code></dt>
<dd>[description].
This Class will be instatiated to get a mapping model. E.g. LinearRegression, Ridge,
from the sklearn family. Must implement &lt;?classifier&gt; interface</dd>
<dt><strong><code>random_seed</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>[description]. Defaults to 42.</dd>
<dt><strong><code>k_fold</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>[description]. Defaults to 5.</dd>
<dt><strong><code>strat_coord</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>[description]. Defaults to None.</dd>
<dt><strong><code>num_split_groups_out</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>[description]. Defaults to None.</dd>
<dt><strong><code>split_coord</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>[description]. Defaults to None.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Mapping:
    model = None

    def __init__(self,
                 X: xr.DataArray, Y: xr.DataArray,

                 mapping_class: typing.Union[str, typing.Any] = None,
                 random_seed: int = 42, 

                 k_fold: int = 5,
                 strat_coord: str = None,

                 num_split_groups_out: int = None, # (p, the # of groups in the test split)
                 split_coord: str = None, # (grouping coord)

                 #TODO
                 # handle predict held-out subject # but then we have to do mean over ROIs
                 # because individual neuroids do not correspond
                 # we kind of already have this along the `sampleid` coordinate, but we
                 # need to implement this in the neuroid coordinate

                 **kwargs) -&gt; None:
        &#34;&#34;&#34;Initializes a Mapping object that establishes a mapping between two encoder representations.
           The mapping is initialized with certain parameters baked in, accepted as arguments to
           the init function, listed below.

        Args:
            mapping_class (typing.Union[str, typing.Any], required): [description]. 
                This Class will be instatiated to get a mapping model. E.g. LinearRegression, Ridge,
                from the sklearn family. Must implement &lt;?classifier&gt; interface
            random_seed (int, optional): [description]. Defaults to 42.
            k_fold (int, optional): [description]. Defaults to 5.
            strat_coord (str, optional): [description]. Defaults to None.
            num_split_groups_out (int, optional): [description]. Defaults to None.
            split_coord (str, optional): [description]. Defaults to None.
        &#34;&#34;&#34;
        self.random_seed = random_seed
        mapping_classes = {
            &#39;ridge&#39;: (Ridge, {&#39;alpha&#39;: 1.0}),
            &#39;ridge_cv&#39;: (RidgeCV, {&#39;alphas&#39;: np.logspace(-3, 3, 13), &#39;alpha_per_target&#39;: True}),
            &#39;linear&#39;: (LinearRegression, {}),
            None: None,
        }

        self.k_fold = k_fold or 1
        self.strat_coord = strat_coord

        self.num_split_groups_out = num_split_groups_out
        self.split_coord = split_coord

        self.mapping_class = mapping_class

        if strat_coord:
            try:
                assert (X[strat_coord].values == Y[strat_coord].values).all()
            except AssertionError as e:
                raise ValueError(f&#39;{strat_coord} coordinate does not align across X and Y&#39;)
        if split_coord:
            try:
                assert (X[split_coord].values == Y[split_coord].values).all() 
            except AssertionError as e:
                raise ValueError(f&#39;{split_coord} coordinate does not align across X and Y&#39;)
        
        # TODO:
        # make sure there are no stimuli that have NaNs in all places along the neuroid dimension

        self.X, self.Y = X, Y
        
        
        
        self.X_nan_mask = X.isnull()
        self.Y_nan_mask = Y.isnull()
        self.X_nan_removed = X.dropna(&#39;neuroid&#39;)
        self.Y_nan_removed = Y.dropna(&#39;neuroid&#39;)

        logging.log(f&#39;X shape: {X.data.shape}, NaNs: {self.X_nan_mask.sum()}; after NaN removal: {self.X_nan_removed.data.shape}&#39;)
        logging.log(f&#39;Y shape: {Y.data.shape}, NaNs: {self.Y_nan_mask.sum()}; after NaN removal: {self.Y_nan_removed.data.shape}&#39;)

        if type(mapping_class) == str:
            mapping_class, _kwargs = mapping_classes[mapping_class]
            kwargs.update(_kwargs)
        
        # to save (this model uses the entire data rather than constructing splits)
        self.full_model = mapping_class(**kwargs)
        # placeholder model with the right params that we&#39;ll reuse across splits
        self.model = mapping_class(**kwargs)
        
        logging.log(f&#39;initialized Mapping with {mapping_class}, {type(self.model)}!&#39;)


    @staticmethod
    def _extract_dense(A = None):
        &#39;&#39;&#39;
        returns a list of several xarrays each of which is dense (has no NaNs). 
        each will have a subset of the sampleids

        Args:
            A (xr.DataArray): 
        &#39;&#39;&#39;

    def extract_dense(self):
        dense_X = self._extract_dense_arrays(self.X)


    @staticmethod
    def _construct_splits(xr_dataset: xr.Dataset, # Y: xr.Dataset, 
                          strat_coord: str = None, k_folds: int = 5,
                          split_coord: str = None, num_split_groups_out: int = None,
                          random_seed: int = 42
                         ):

        sampleid = xr_dataset.sampleid.values

        if strat_coord and split_coord:
            kf = StratifiedGroupKFold(n_splits=k_folds, shuffle=True, random_state=random_seed)
            split = partial(kf.split, sampleid, y=xr_dataset[split_coord].values, groups=xr_dataset[strat_coord].values)
        elif split_coord:
            kf = GroupKFold(n_splits=k_folds)
            split = partial(kf.split, sampleid, groups=xr_dataset[split_coord].values)
        elif strat_coord:
            kf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=random_seed)
            split = partial(kf.split, sampleid, y=xr_dataset[strat_coord].values)
        else:
            kf = KFold(n_splits=k_folds, shuffle=True, random_state=random_seed)
            split = partial(kf.split, sampleid)

        logging.log(f&#39;running {type(kf)}!&#39;)
        return split()


    def construct_splits(self, A):
        return self._construct_splits(A,
                                      self.strat_coord, self.k_fold, 
                                      self.split_coord, self.num_split_groups_out,
                                      random_seed=self.random_seed)

        
    def fit_full(self, X, Y):
        # TODO
        self.fit(X, Y, k_folds=1)
        raise NotImplemented

    def fit(self, 
            #X: xr.Dataset, Y: xr.Dataset
           ) -&gt; None:
        &#34;&#34;&#34;creates a mapping model using k-fold cross-validation
            -&gt; uses params from the class initialization, uses strat_coord
               and split_coord to stratify and split across group boundaries

        Returns:
            [type]: [description]
        &#34;&#34;&#34;        

        for neuroid in self.Y.neuroid.values:

            # limit data to current neuroid, and then drop the samples that are missing data for this neuroid
            Y_slice = self.Y.sel(neuroid=neuroid).dropna(dim=&#39;sampleid&#39;)
            Y_filtered_sampleids = Y_slice.sampleid
            assert set(Y_filtered_sampleids.values).issubset(set(self.X.sampleid.values))
            logging.log(f&#39;for neuroid {neuroid}, we used {(num_retained := len(Y_filtered_sampleids))} samples; dropped {len(self.Y.sampleid) - num_retained}&#39;) 

            X_slice = self.X.sel(sampleid=Y_filtered_sampleids.values)

            # these collections store each split for our records later
            alpha_across_splits = [] # only used in case of ridge_cv # TODO
            # TODO we aren&#39;t saving this to the object instance yet
            train_indices = []
            test_indices = []

            splits = self.construct_splits(Y_slice)

            # X_test_collection = []
            Y_test_collection = []
            Y_pred_collection = []

            for train_index, test_index in splits:
                
                train_indices.append(train_index)
                test_indices.append(test_index)

                # !! NOTE the _nan_removed variants instead of X and Y
                X_train, X_test = (
                    X_slice.sel(sampleid=Y_slice.sampleid.values[train_index]),
                    X_slice.sel(sampleid=Y_slice.sampleid.values[test_index]),
                )
                y_train, y_test = (
                    Y_slice.sel(sampleid=Y_slice.sampleid.values[train_index]),
                    Y_slice.sel(sampleid=Y_slice.sampleid.values[test_index]),
                )

                y_pred_over_time = []
                for timeid in y_train.timeid:

                    # TODO: change this code for models that also have a non-singleton timeid
                    # i.e., output evolves in time (RNN?)
                    self.model.fit(X_train.sel(timeid=0), y_train.sel(timeid=timeid).values.reshape(-1, 1))

                    # deepcopy
                    y_pred = y_test.sel(timeid=timeid).copy(deep=True).expand_dims(&#39;timeid&#39;, 1)
                    y_pred.assign_coords(timeid=(&#39;timeid&#39;, [timeid]))
                    y_pred.data = self.model.predict(X_test.sel(timeid=0)) #y_pred
                    y_pred_over_time.append(y_pred)

                y_pred_over_time = xr.concat(y_pred_over_time, dim=&#39;timeid&#39;)
                Y_pred_collection.append(y_pred_over_time)

                Y_test_collection.append(y_test)

            # ACTUALLY TODO the below is no longer true:
            #   now the Y_test_collection members are xarrays with a timeid dimension/coord
            #   but Y_pred_collection is a list of numpy arrays per discrete timeid

            # the return value is a dictionary of test/pred;
            # each of test/pred is a list of lists with two levels of
            # nesting as below:
            #   first level: CV folds
            #       second level: timeids
            yield dict(test=Y_test_collection, 
                       pred=Y_pred_collection)

    
    # def map(self, source, target) -&gt; None:
    #     &#39;&#39;&#39;
    #     the works: constructs splits, fits models for each split, then evaluates the fit 
    #             of each split and returns the result (also for each split)
    #     &#39;&#39;&#39;
    #     pass

        
    def save_model(self) -&gt; None:
        &#39;&#39;&#39;TODO: stuff that needs to be saved eventually

        - model weights
        - CV stuff (if using CV); but all arguments needed for initializing, in general
            - n_splits
            - random_state
            - split indices (based on random seed)
            - params per split (alpha, lambda)
            - validation score, etc. for each CV split?
        &#39;&#39;&#39;
        pass

    def predict(self, source) -&gt; None:
        pass</code></pre>
</details>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="langbrainscore.mapping.mapping.IdentityMap" href="#langbrainscore.mapping.mapping.IdentityMap">IdentityMap</a></li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="langbrainscore.mapping.mapping.Mapping.model"><code class="name">var <span class="ident">model</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="langbrainscore.mapping.mapping.Mapping.construct_splits"><code class="name flex">
<span>def <span class="ident">construct_splits</span></span>(<span>self, A)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def construct_splits(self, A):
    return self._construct_splits(A,
                                  self.strat_coord, self.k_fold, 
                                  self.split_coord, self.num_split_groups_out,
                                  random_seed=self.random_seed)</code></pre>
</details>
</dd>
<dt id="langbrainscore.mapping.mapping.Mapping.extract_dense"><code class="name flex">
<span>def <span class="ident">extract_dense</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def extract_dense(self):
    dense_X = self._extract_dense_arrays(self.X)</code></pre>
</details>
</dd>
<dt id="langbrainscore.mapping.mapping.Mapping.fit"><code class="name flex">
<span>def <span class="ident">fit</span></span>(<span>self) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>creates a mapping model using k-fold cross-validation
-&gt; uses params from the class initialization, uses strat_coord
and split_coord to stratify and split across group boundaries</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>[type]</code></dt>
<dd>[description]</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fit(self, 
        #X: xr.Dataset, Y: xr.Dataset
       ) -&gt; None:
    &#34;&#34;&#34;creates a mapping model using k-fold cross-validation
        -&gt; uses params from the class initialization, uses strat_coord
           and split_coord to stratify and split across group boundaries

    Returns:
        [type]: [description]
    &#34;&#34;&#34;        

    for neuroid in self.Y.neuroid.values:

        # limit data to current neuroid, and then drop the samples that are missing data for this neuroid
        Y_slice = self.Y.sel(neuroid=neuroid).dropna(dim=&#39;sampleid&#39;)
        Y_filtered_sampleids = Y_slice.sampleid
        assert set(Y_filtered_sampleids.values).issubset(set(self.X.sampleid.values))
        logging.log(f&#39;for neuroid {neuroid}, we used {(num_retained := len(Y_filtered_sampleids))} samples; dropped {len(self.Y.sampleid) - num_retained}&#39;) 

        X_slice = self.X.sel(sampleid=Y_filtered_sampleids.values)

        # these collections store each split for our records later
        alpha_across_splits = [] # only used in case of ridge_cv # TODO
        # TODO we aren&#39;t saving this to the object instance yet
        train_indices = []
        test_indices = []

        splits = self.construct_splits(Y_slice)

        # X_test_collection = []
        Y_test_collection = []
        Y_pred_collection = []

        for train_index, test_index in splits:
            
            train_indices.append(train_index)
            test_indices.append(test_index)

            # !! NOTE the _nan_removed variants instead of X and Y
            X_train, X_test = (
                X_slice.sel(sampleid=Y_slice.sampleid.values[train_index]),
                X_slice.sel(sampleid=Y_slice.sampleid.values[test_index]),
            )
            y_train, y_test = (
                Y_slice.sel(sampleid=Y_slice.sampleid.values[train_index]),
                Y_slice.sel(sampleid=Y_slice.sampleid.values[test_index]),
            )

            y_pred_over_time = []
            for timeid in y_train.timeid:

                # TODO: change this code for models that also have a non-singleton timeid
                # i.e., output evolves in time (RNN?)
                self.model.fit(X_train.sel(timeid=0), y_train.sel(timeid=timeid).values.reshape(-1, 1))

                # deepcopy
                y_pred = y_test.sel(timeid=timeid).copy(deep=True).expand_dims(&#39;timeid&#39;, 1)
                y_pred.assign_coords(timeid=(&#39;timeid&#39;, [timeid]))
                y_pred.data = self.model.predict(X_test.sel(timeid=0)) #y_pred
                y_pred_over_time.append(y_pred)

            y_pred_over_time = xr.concat(y_pred_over_time, dim=&#39;timeid&#39;)
            Y_pred_collection.append(y_pred_over_time)

            Y_test_collection.append(y_test)

        # ACTUALLY TODO the below is no longer true:
        #   now the Y_test_collection members are xarrays with a timeid dimension/coord
        #   but Y_pred_collection is a list of numpy arrays per discrete timeid

        # the return value is a dictionary of test/pred;
        # each of test/pred is a list of lists with two levels of
        # nesting as below:
        #   first level: CV folds
        #       second level: timeids
        yield dict(test=Y_test_collection, 
                   pred=Y_pred_collection)</code></pre>
</details>
</dd>
<dt id="langbrainscore.mapping.mapping.Mapping.fit_full"><code class="name flex">
<span>def <span class="ident">fit_full</span></span>(<span>self, X, Y)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fit_full(self, X, Y):
    # TODO
    self.fit(X, Y, k_folds=1)
    raise NotImplemented</code></pre>
</details>
</dd>
<dt id="langbrainscore.mapping.mapping.Mapping.predict"><code class="name flex">
<span>def <span class="ident">predict</span></span>(<span>self, source) ‑> None</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def predict(self, source) -&gt; None:
    pass</code></pre>
</details>
</dd>
<dt id="langbrainscore.mapping.mapping.Mapping.save_model"><code class="name flex">
<span>def <span class="ident">save_model</span></span>(<span>self) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>TODO: stuff that needs to be saved eventually</p>
<ul>
<li>model weights</li>
<li>CV stuff (if using CV); but all arguments needed for initializing, in general<ul>
<li>n_splits</li>
<li>random_state</li>
<li>split indices (based on random seed)</li>
<li>params per split (alpha, lambda)</li>
<li>validation score, etc. for each CV split?</li>
</ul>
</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def save_model(self) -&gt; None:
    &#39;&#39;&#39;TODO: stuff that needs to be saved eventually

    - model weights
    - CV stuff (if using CV); but all arguments needed for initializing, in general
        - n_splits
        - random_state
        - split indices (based on random seed)
        - params per split (alpha, lambda)
        - validation score, etc. for each CV split?
    &#39;&#39;&#39;
    pass</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="langbrainscore.mapping" href="index.html">langbrainscore.mapping</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="langbrainscore.mapping.mapping.IdentityMap" href="#langbrainscore.mapping.mapping.IdentityMap">IdentityMap</a></code></h4>
<ul class="">
<li><code><a title="langbrainscore.mapping.mapping.IdentityMap.fit" href="#langbrainscore.mapping.mapping.IdentityMap.fit">fit</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="langbrainscore.mapping.mapping.Mapping" href="#langbrainscore.mapping.mapping.Mapping">Mapping</a></code></h4>
<ul class="two-column">
<li><code><a title="langbrainscore.mapping.mapping.Mapping.construct_splits" href="#langbrainscore.mapping.mapping.Mapping.construct_splits">construct_splits</a></code></li>
<li><code><a title="langbrainscore.mapping.mapping.Mapping.extract_dense" href="#langbrainscore.mapping.mapping.Mapping.extract_dense">extract_dense</a></code></li>
<li><code><a title="langbrainscore.mapping.mapping.Mapping.fit" href="#langbrainscore.mapping.mapping.Mapping.fit">fit</a></code></li>
<li><code><a title="langbrainscore.mapping.mapping.Mapping.fit_full" href="#langbrainscore.mapping.mapping.Mapping.fit_full">fit_full</a></code></li>
<li><code><a title="langbrainscore.mapping.mapping.Mapping.model" href="#langbrainscore.mapping.mapping.Mapping.model">model</a></code></li>
<li><code><a title="langbrainscore.mapping.mapping.Mapping.predict" href="#langbrainscore.mapping.mapping.Mapping.predict">predict</a></code></li>
<li><code><a title="langbrainscore.mapping.mapping.Mapping.save_model" href="#langbrainscore.mapping.mapping.Mapping.save_model">save_model</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>