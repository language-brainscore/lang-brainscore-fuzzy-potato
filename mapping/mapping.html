<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>langbrainscore.mapping.mapping API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>langbrainscore.mapping.mapping</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import typing
from functools import partial

from tqdm.auto import tqdm
import numpy as np
from joblib import Parallel, delayed
import xarray as xr
from sklearn.cross_decomposition import PLSRegression
from sklearn.linear_model import LinearRegression, RidgeCV

from langbrainscore.interface import _Mapping
from langbrainscore.utils import logging
from langbrainscore.utils.xarray import collapse_multidim_coord

mapping_classes_params = {
    &#34;linreg&#34;: (LinearRegression, {}),
    &#34;linridge_cv&#34;: (RidgeCV, {&#34;alphas&#34;: np.logspace(-3, 3, 13)}),
    &#34;linpls&#34;: (PLSRegression, {&#34;n_components&#34;: 20}),
}


class IdentityMap(_Mapping):
    &#34;&#34;&#34;
    Identity mapping for use with metrics that operate
    on non column-aligned matrices, e.g., RSA, CKA

    Imputes NaNs for downstream metrics.
    &#34;&#34;&#34;

    def __init__(self, nan_strategy: str = &#34;drop&#34;) -&gt; &#34;IdentityMap&#34;:
        self._nan_strategy = nan_strategy

    def fit_transform(
        self,
        X: xr.DataArray,
        Y: xr.DataArray,
        ceiling: bool = False,
    ):
        if ceiling:
            logging.log(&#34;ceiling not supported for IdentityMap yet&#34;)
        # TODO: figure out how to handle NaNs better...
        if self._nan_strategy == &#34;drop&#34;:
            X_clean = X.copy(deep=True).dropna(dim=&#34;neuroid&#34;)
            Y_clean = Y.copy(deep=True).dropna(dim=&#34;neuroid&#34;)
        elif self._nan_strategy == &#34;impute&#34;:
            X_clean = X.copy(deep=True).fillna(0)
            Y_clean = Y.copy(deep=True).fillna(0)
        else:
            raise NotImplementedError(&#34;unsupported nan strategy.&#34;)
        return X_clean, Y_clean


class LearnedMap(_Mapping):
    def __init__(
        self,
        mapping_class: typing.Union[
            str, typing.Tuple[typing.Callable, typing.Mapping[str, typing.Any]]
        ],
        random_seed: int = 42,
        k_fold: int = 5,
        strat_coord: str = None,
        num_split_groups_out: int = None,  # (p, the # of groups in the test split)
        split_coord: str = None,  # (grouping coord)
        # TODO
        # handle predict held-out subject # but then we have to do mean over ROIs
        # because individual neuroids do not correspond
        # we kind of already have this along the `sampleid` coordinate, but we
        # need to implement this in the neuroid coordinate
        **kwargs,
    ) -&gt; &#34;LearnedMap&#34;:
        &#34;&#34;&#34;
        Initializes a Mapping object that describes a mapping between two encoder representations.

        Args:
            mapping_class (typing.Union[str, typing.Any], required): [description].
                This Class will be instatiated to get a mapping model. E.g. LinearRegression, Ridge,
                from the sklearn family. Must implement &lt;?classifier&gt; interface
            random_seed (int, optional): [description]. Defaults to 42.
            k_fold (int, optional): [description]. Defaults to 5.
            strat_coord (str, optional): [description]. Defaults to None.
            num_split_groups_out (int, optional): [description]. Defaults to None.
            split_coord (str, optional): [description]. Defaults to None.
        &#34;&#34;&#34;
        self.random_seed = random_seed
        self.k_fold = k_fold
        self.strat_coord = strat_coord
        self.num_split_groups_out = num_split_groups_out
        self.split_coord = split_coord
        self.mapping_class_name = mapping_class
        self.mapping_params = kwargs

        if type(mapping_class) == str:
            _mapping_class, _kwargs = mapping_classes_params[self.mapping_class_name]
            self.mapping_params.update(_kwargs)
        # in the spirit of duck-typing, we don&#39;t need any of these checks. we will automatically
        # fail if we&#39;re missing any of these attributes
        # else:
        #     assert callable(mapping_class)
        #     assert hasattr(mapping_class(), &#34;fit&#34;)
        #     assert hasattr(mapping_class(), &#34;predict&#34;)

        # TODO: what is the difference between these two (model; full_model)? let&#39;s make this less
        # confusing
        self.full_model = _mapping_class(**self.mapping_params)
        self.model = _mapping_class(**self.mapping_params)
        logging.log(f&#34;initialized Mapping with {type(self.model)}!&#34;)

    @staticmethod
    def _construct_splits(
        xr_dataset: xr.Dataset,
        strat_coord: str,
        k_folds: int,
        split_coord: str,
        num_split_groups_out: int,
        random_seed: int,
    ):
        from sklearn.model_selection import (
            GroupKFold,
            KFold,
            StratifiedGroupKFold,
            StratifiedKFold,
        )

        sampleid = xr_dataset.sampleid.values

        if strat_coord and split_coord:
            kf = StratifiedGroupKFold(
                n_splits=k_folds, shuffle=True, random_state=random_seed
            )
            split = partial(
                kf.split,
                sampleid,
                y=xr_dataset[split_coord].values,
                groups=xr_dataset[strat_coord].values,
            )
        elif split_coord:
            kf = GroupKFold(n_splits=k_folds)
            split = partial(kf.split, sampleid, groups=xr_dataset[split_coord].values)
        elif strat_coord:
            kf = StratifiedKFold(
                n_splits=k_folds, shuffle=True, random_state=random_seed
            )
            split = partial(kf.split, sampleid, y=xr_dataset[strat_coord].values)
        else:
            kf = KFold(n_splits=k_folds, shuffle=True, random_state=random_seed)
            split = partial(kf.split, sampleid)

        logging.log(f&#34;running {type(kf)}!&#34;, verbosity_check=True)
        return split()

    def construct_splits(self, A):
        return self._construct_splits(
            A,
            self.strat_coord,
            self.k_fold,
            self.split_coord,
            self.num_split_groups_out,
            random_seed=self.random_seed,
        )

    def fit_full(self, X, Y):
        # TODO
        self.fit(X, Y, k_folds=1)
        raise NotImplemented

    def _check_sampleids(
        self,
        X: xr.DataArray,
        Y: xr.DataArray,
    ):
        &#34;&#34;&#34;
        checks that the sampleids in X and Y are the same
        &#34;&#34;&#34;

        if X.sampleid.values.shape != Y.sampleid.values.shape:
            raise ValueError(&#34;X and Y sampleid shapes do not match!&#34;)
        if not np.all(X.sampleid.values == Y.sampleid.values):
            raise ValueError(&#34;X and Y sampleids do not match!&#34;)

        logging.log(
            f&#34;Passed sampleid check for neuroid {Y.neuroid.values}&#34;,
            verbosity_check=True,
        )

    def _drop_na(
        self, X: xr.DataArray, Y: xr.DataArray, dim: str = &#34;sampleid&#34;, **kwargs
    ):
        &#34;&#34;&#34;
        drop samples with missing values (based on Y) in X or Y along specified dimension
        Make sure that X and Y now have the same sampleids
        &#34;&#34;&#34;
        # limit data to current neuroid, and then drop the samples that are missing data for this neuroid
        Y_slice = Y.dropna(dim=dim, **kwargs)
        Y_filtered_ids = Y_slice[dim].values

        assert set(Y_filtered_ids).issubset(set(X[dim].values))

        logging.log(
            f&#34;for neuroid {Y_slice.neuroid.values}, we used {(num_retained := len(Y_filtered_ids))}&#34;
            f&#34; samples; dropped {len(Y[dim]) - num_retained}&#34;,
            verbosity_check=True,
        )

        # use only the samples that are in Y
        X_slice = X.sel(sampleid=Y_filtered_ids)

        return X_slice, Y_slice

    # def _permute_X(
    #     self,
    #     X: xr.DataArray,
    #     method: str = &#34;shuffle_X_rows&#34;,
    #     random_state: int = 42,
    # ):
    #     &#34;&#34;&#34;Permute the features of X.
    #
    #     Parameters
    #     ----------
    #     X : xr.DataArray
    #             The embeddings to be permuted
    #     method : str
    #             The method to use for permutation.
    #             &#39;shuffle_X_rows&#39; : Shuffle the rows of X (=shuffle the sentences and create a mismatch between the sentence embeddings and target)
    #             &#39;shuffle_each_X_col&#39;: For each column (=feature/unit) of X, permute that feature&#39;s values across all sentences.
    #                                                       Retains the statistics of the original features (e.g., mean per feature) but the values of the features are shuffled for each sentence.
    #     random_state : int
    #             The seed for the random number generator.
    #
    #     Returns
    #     -------
    #     xr.DataArray
    #             The permuted dataarray
    #     &#34;&#34;&#34;
    #
    #     X_orig = X.copy(deep=True)
    #
    #     if logging.get_verbosity():
    #         logging.log(f&#34;OBS: permuting X with method {method}&#34;)
    #
    #     if method == &#34;shuffle_X_rows&#34;:
    #         X = X.sample(
    #             n=X.shape[1], random_state=random_state
    #         )  # check whether X_shape is correct
    #
    #     elif method == &#34;shuffle_each_X_col&#34;:
    #         np.random.seed(random_state)
    #         for feat in X.data.shape[0]:  # per neuroid
    #             np.random.shuffle(X.data[feat, :])
    #
    #     else:
    #         raise ValueError(f&#34;Invalid method: {method}&#34;)
    #
    #     assert X.shape == X_orig.shape
    #     assert np.all(X.data != X_orig.data)
    #
    #     return X

    def fit_transform(
        self,
        X: xr.DataArray,
        Y: xr.DataArray,
        # permute_X: typing.Union[bool, str] = False,
        ceiling: bool = False,
        ceiling_coord: str = &#34;subject&#34;,
    ) -&gt; typing.Tuple[xr.DataArray, xr.DataArray]:
        &#34;&#34;&#34;creates a mapping model using k-fold cross-validation
            -&gt; uses params from the class initialization, uses strat_coord
               and split_coord to stratify and split across group boundaries

        Returns:
            [type]: [description]
        &#34;&#34;&#34;
        from sklearn.random_projection import GaussianRandomProjection

        if ceiling:
            n_neuroids = X.neuroid.values.size
            X = Y.copy()

        logging.log(f&#34;X shape: {X.data.shape}&#34;, verbosity_check=True)
        logging.log(f&#34;Y shape: {Y.data.shape}&#34;, verbosity_check=True)

        if self.strat_coord:
            try:
                assert (X[self.strat_coord].values == Y[self.strat_coord].values).all()
            except AssertionError as e:
                raise ValueError(
                    f&#34;{self.strat_coord} coordinate does not align across X and Y&#34;
                )
        if self.split_coord:
            try:
                assert (X[self.split_coord].values == Y[self.split_coord].values).all()
            except AssertionError as e:
                raise ValueError(
                    f&#34;{self.split_coord} coordinate does not align across X and Y&#34;
                )

        def fit_per_neuroid(neuroid):
            Y_neuroid = Y.sel(neuroid=neuroid)

            # limit data to current neuroid, and then drop the samples that are missing data for this neuroid
            X_slice, Y_slice = self._drop_na(X, Y_neuroid, dim=&#34;sampleid&#34;)

            # Assert that X and Y have the same sampleids
            self._check_sampleids(X_slice, Y_slice)

            # select relevant ceiling split
            if ceiling:
                X_slice = X_slice.isel(
                    neuroid=X_slice[ceiling_coord] != Y_slice[ceiling_coord]
                ).dropna(dim=&#34;neuroid&#34;)

            # We can perform various sanity checks by &#39;permuting&#39; the source, X
            # NOTE this is a test! do not use under normal workflow!
            # if permute_X:
            #     logging.log(
            #         f&#34;`permute_X` flag is enabled. only do this in an adversarial setting.&#34;,
            #         cmap=&#34;WARN&#34;,
            #         type=&#34;WARN&#34;,
            #         verbosity_check=True,
            #     )
            #     X_slice = self._permute_X(X_slice, method=permute_X)

            # these collections store each split for our records later
            # TODO we aren&#39;t saving this to the object instance yet
            train_indices = []
            test_indices = []
            # only used in case of ridge_cv or any duck type that uses an alpha hparam

            splits = self.construct_splits(Y_slice)

            # X_test_collection = []
            Y_test_collection = []
            Y_pred_collection = []

            for cvfoldid, (train_index, test_index) in enumerate(splits):

                train_indices.append(train_index)
                test_indices.append(test_index)

                # !! NOTE the _nan_removed variants instead of X and Y
                X_train, X_test = (
                    X_slice.sel(sampleid=Y_slice.sampleid.values[train_index]),
                    X_slice.sel(sampleid=Y_slice.sampleid.values[test_index]),
                )
                y_train, y_test = (
                    Y_slice.sel(sampleid=Y_slice.sampleid.values[train_index]),
                    Y_slice.sel(sampleid=Y_slice.sampleid.values[test_index]),
                )

                # empty list to house the y_predictions per timeid
                y_pred_over_time = []

                for timeid in y_train.timeid:

                    # TODO: change this code for models that also have a non-singleton timeid
                    # i.e., output evolves in time (RNN?)

                    x_model_train = X_train.sel(timeid=0).values
                    y_model_train = y_train.sel(timeid=timeid).values.reshape(-1, 1)

                    if ceiling and x_model_train.shape[1] &gt; n_neuroids:
                        projection = GaussianRandomProjection(
                            n_components=n_neuroids, random_state=0
                        )
                        x_model_train = projection.fit_transform(x_model_train)

                    self.model.fit(
                        x_model_train,
                        y_model_train,
                    )

                    # store the hparam values related to the fitted models
                    alpha = getattr(self.model, &#34;alpha_&#34;, np.nan)

                    # deepcopy `y_test` as `y_pred` to inherit some of the metadata and dims
                    # and then populate it with our new predicted values
                    y_pred = (
                        y_test.sel(timeid=timeid)
                        .copy(deep=True)
                        .expand_dims(&#34;timeid&#34;, 1)
                    )
                    x_model_test = X_test.sel(timeid=0)
                    if ceiling and x_model_train.shape[1] &gt; n_neuroids:
                        x_model_test = projection.transform(x_model_test)
                    y_pred.data = self.model.predict(x_model_test)  # y_pred
                    y_pred = y_pred.assign_coords(timeid=(&#34;timeid&#34;, [timeid]))
                    y_pred = y_pred.assign_coords(alpha=(&#34;timeid&#34;, [alpha]))
                    y_pred = y_pred.assign_coords(cvfoldid=(&#34;timeid&#34;, [cvfoldid]))
                    y_pred_over_time.append(y_pred)

                y_pred_over_time = xr.concat(y_pred_over_time, dim=&#34;timeid&#34;)
                Y_pred_collection.append(y_pred_over_time)
                Y_test_collection.append(y_test)

            Y_test = xr.concat(Y_test_collection, dim=&#34;sampleid&#34;).sortby(&#34;sampleid&#34;)
            Y_pred = xr.concat(Y_pred_collection, dim=&#34;sampleid&#34;).sortby(&#34;sampleid&#34;)

            # test.append(Y_test)
            # pred.append(Y_pred)
            return Y_test, Y_pred

        # Loop across each Y neuroid (target)
        test = []
        pred = []
        # TODO: parallelize using order-preserving joblib-mapping
        # for neuroid in tqdm(Y.neuroid.values, desc=&#34;fitting a model per neuroid&#34;):
        for t, p in Parallel(n_jobs=-2)(
            delayed(fit_per_neuroid)(neuroid)
            for neuroid in tqdm(Y.neuroid.values, desc=&#34;fitting a model per neuroid&#34;)
        ):
            test += [t]
            pred += [p]

        test_xr = xr.concat(test, dim=&#34;neuroid&#34;).transpose(
            &#34;sampleid&#34;, &#34;neuroid&#34;, &#34;timeid&#34;
        )
        pred_xr = xr.concat(pred, dim=&#34;neuroid&#34;).transpose(
            &#34;sampleid&#34;, &#34;neuroid&#34;, &#34;timeid&#34;
        )

        if test_xr.stimulus.ndim &gt; 1:
            test_xr = collapse_multidim_coord(test_xr, &#34;stimulus&#34;, &#34;sampleid&#34;)
        if pred_xr.stimulus.ndim &gt; 1:
            pred_xr = collapse_multidim_coord(pred_xr, &#34;stimulus&#34;, &#34;sampleid&#34;)

        return pred_xr, test_xr

    # def map(self, source, target) -&gt; None:
    #     &#39;&#39;&#39;
    #     the works: constructs splits, fits models for each split, then evaluates the fit
    #             of each split and returns the result (also for each split)
    #     &#39;&#39;&#39;
    #     pass

    def save_model(self) -&gt; None:
        &#34;&#34;&#34;TODO: stuff that needs to be saved eventually

        - model weights
        - CV stuff (if using CV); but all arguments needed for initializing, in general
            - n_splits
            - random_state
            - split indices (based on random seed)
            - params per split (alpha, lambda)
            - validation score, etc. for each CV split?
        &#34;&#34;&#34;
        pass

    def predict(self, source) -&gt; None:
        pass</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="langbrainscore.mapping.mapping.IdentityMap"><code class="flex name class">
<span>class <span class="ident">IdentityMap</span></span>
<span>(</span><span>nan_strategy: str = 'drop')</span>
</code></dt>
<dd>
<div class="desc"><p>Identity mapping for use with metrics that operate
on non column-aligned matrices, e.g., RSA, CKA</p>
<p>Imputes NaNs for downstream metrics.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class IdentityMap(_Mapping):
    &#34;&#34;&#34;
    Identity mapping for use with metrics that operate
    on non column-aligned matrices, e.g., RSA, CKA

    Imputes NaNs for downstream metrics.
    &#34;&#34;&#34;

    def __init__(self, nan_strategy: str = &#34;drop&#34;) -&gt; &#34;IdentityMap&#34;:
        self._nan_strategy = nan_strategy

    def fit_transform(
        self,
        X: xr.DataArray,
        Y: xr.DataArray,
        ceiling: bool = False,
    ):
        if ceiling:
            logging.log(&#34;ceiling not supported for IdentityMap yet&#34;)
        # TODO: figure out how to handle NaNs better...
        if self._nan_strategy == &#34;drop&#34;:
            X_clean = X.copy(deep=True).dropna(dim=&#34;neuroid&#34;)
            Y_clean = Y.copy(deep=True).dropna(dim=&#34;neuroid&#34;)
        elif self._nan_strategy == &#34;impute&#34;:
            X_clean = X.copy(deep=True).fillna(0)
            Y_clean = Y.copy(deep=True).fillna(0)
        else:
            raise NotImplementedError(&#34;unsupported nan strategy.&#34;)
        return X_clean, Y_clean</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>langbrainscore.interface.mapping._Mapping</li>
<li>langbrainscore.interface.cacheable._Cacheable</li>
<li>typing.Protocol</li>
<li>typing.Generic</li>
<li>abc.ABC</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="langbrainscore.mapping.mapping.IdentityMap.fit_transform"><code class="name flex">
<span>def <span class="ident">fit_transform</span></span>(<span>self, X: xarray.core.dataarray.DataArray, Y: xarray.core.dataarray.DataArray, ceiling: bool = False)</span>
</code></dt>
<dd>
<div class="desc"><p>takes in two xarrays with a shared set of samples and returns a new
pair of xarrays (Y_pred, Y_true) to be compared with a metric.T</p>
<p>Y_pred is either derived from a learned mapping on X or can be X itself
when the downstream metric supports comparison of matrices with
different dimensions, e.g, RSA, CKA</p>
<p>args:
xr.DataArray: X
xr.DataArray: Y</p>
<p>returns:
xr.DataArray: Y_pred
xr.DataArray: Y_true</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fit_transform(
    self,
    X: xr.DataArray,
    Y: xr.DataArray,
    ceiling: bool = False,
):
    if ceiling:
        logging.log(&#34;ceiling not supported for IdentityMap yet&#34;)
    # TODO: figure out how to handle NaNs better...
    if self._nan_strategy == &#34;drop&#34;:
        X_clean = X.copy(deep=True).dropna(dim=&#34;neuroid&#34;)
        Y_clean = Y.copy(deep=True).dropna(dim=&#34;neuroid&#34;)
    elif self._nan_strategy == &#34;impute&#34;:
        X_clean = X.copy(deep=True).fillna(0)
        Y_clean = Y.copy(deep=True).fillna(0)
    else:
        raise NotImplementedError(&#34;unsupported nan strategy.&#34;)
    return X_clean, Y_clean</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="langbrainscore.mapping.mapping.LearnedMap"><code class="flex name class">
<span>class <span class="ident">LearnedMap</span></span>
<span>(</span><span>mapping_class: Union[str, Tuple[Callable, Mapping[str, Any]]], random_seed: int = 42, k_fold: int = 5, strat_coord: str = None, num_split_groups_out: int = None, split_coord: str = None, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>object that defines and applies map between two xarrays with the same number of samples</p>
<p>Initializes a Mapping object that describes a mapping between two encoder representations.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>mapping_class</code></strong> :&ensp;<code>typing.Union[str, typing.Any], required</code></dt>
<dd>[description].
This Class will be instatiated to get a mapping model. E.g. LinearRegression, Ridge,
from the sklearn family. Must implement &lt;?classifier&gt; interface</dd>
<dt><strong><code>random_seed</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>[description]. Defaults to 42.</dd>
<dt><strong><code>k_fold</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>[description]. Defaults to 5.</dd>
<dt><strong><code>strat_coord</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>[description]. Defaults to None.</dd>
<dt><strong><code>num_split_groups_out</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>[description]. Defaults to None.</dd>
<dt><strong><code>split_coord</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>[description]. Defaults to None.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class LearnedMap(_Mapping):
    def __init__(
        self,
        mapping_class: typing.Union[
            str, typing.Tuple[typing.Callable, typing.Mapping[str, typing.Any]]
        ],
        random_seed: int = 42,
        k_fold: int = 5,
        strat_coord: str = None,
        num_split_groups_out: int = None,  # (p, the # of groups in the test split)
        split_coord: str = None,  # (grouping coord)
        # TODO
        # handle predict held-out subject # but then we have to do mean over ROIs
        # because individual neuroids do not correspond
        # we kind of already have this along the `sampleid` coordinate, but we
        # need to implement this in the neuroid coordinate
        **kwargs,
    ) -&gt; &#34;LearnedMap&#34;:
        &#34;&#34;&#34;
        Initializes a Mapping object that describes a mapping between two encoder representations.

        Args:
            mapping_class (typing.Union[str, typing.Any], required): [description].
                This Class will be instatiated to get a mapping model. E.g. LinearRegression, Ridge,
                from the sklearn family. Must implement &lt;?classifier&gt; interface
            random_seed (int, optional): [description]. Defaults to 42.
            k_fold (int, optional): [description]. Defaults to 5.
            strat_coord (str, optional): [description]. Defaults to None.
            num_split_groups_out (int, optional): [description]. Defaults to None.
            split_coord (str, optional): [description]. Defaults to None.
        &#34;&#34;&#34;
        self.random_seed = random_seed
        self.k_fold = k_fold
        self.strat_coord = strat_coord
        self.num_split_groups_out = num_split_groups_out
        self.split_coord = split_coord
        self.mapping_class_name = mapping_class
        self.mapping_params = kwargs

        if type(mapping_class) == str:
            _mapping_class, _kwargs = mapping_classes_params[self.mapping_class_name]
            self.mapping_params.update(_kwargs)
        # in the spirit of duck-typing, we don&#39;t need any of these checks. we will automatically
        # fail if we&#39;re missing any of these attributes
        # else:
        #     assert callable(mapping_class)
        #     assert hasattr(mapping_class(), &#34;fit&#34;)
        #     assert hasattr(mapping_class(), &#34;predict&#34;)

        # TODO: what is the difference between these two (model; full_model)? let&#39;s make this less
        # confusing
        self.full_model = _mapping_class(**self.mapping_params)
        self.model = _mapping_class(**self.mapping_params)
        logging.log(f&#34;initialized Mapping with {type(self.model)}!&#34;)

    @staticmethod
    def _construct_splits(
        xr_dataset: xr.Dataset,
        strat_coord: str,
        k_folds: int,
        split_coord: str,
        num_split_groups_out: int,
        random_seed: int,
    ):
        from sklearn.model_selection import (
            GroupKFold,
            KFold,
            StratifiedGroupKFold,
            StratifiedKFold,
        )

        sampleid = xr_dataset.sampleid.values

        if strat_coord and split_coord:
            kf = StratifiedGroupKFold(
                n_splits=k_folds, shuffle=True, random_state=random_seed
            )
            split = partial(
                kf.split,
                sampleid,
                y=xr_dataset[split_coord].values,
                groups=xr_dataset[strat_coord].values,
            )
        elif split_coord:
            kf = GroupKFold(n_splits=k_folds)
            split = partial(kf.split, sampleid, groups=xr_dataset[split_coord].values)
        elif strat_coord:
            kf = StratifiedKFold(
                n_splits=k_folds, shuffle=True, random_state=random_seed
            )
            split = partial(kf.split, sampleid, y=xr_dataset[strat_coord].values)
        else:
            kf = KFold(n_splits=k_folds, shuffle=True, random_state=random_seed)
            split = partial(kf.split, sampleid)

        logging.log(f&#34;running {type(kf)}!&#34;, verbosity_check=True)
        return split()

    def construct_splits(self, A):
        return self._construct_splits(
            A,
            self.strat_coord,
            self.k_fold,
            self.split_coord,
            self.num_split_groups_out,
            random_seed=self.random_seed,
        )

    def fit_full(self, X, Y):
        # TODO
        self.fit(X, Y, k_folds=1)
        raise NotImplemented

    def _check_sampleids(
        self,
        X: xr.DataArray,
        Y: xr.DataArray,
    ):
        &#34;&#34;&#34;
        checks that the sampleids in X and Y are the same
        &#34;&#34;&#34;

        if X.sampleid.values.shape != Y.sampleid.values.shape:
            raise ValueError(&#34;X and Y sampleid shapes do not match!&#34;)
        if not np.all(X.sampleid.values == Y.sampleid.values):
            raise ValueError(&#34;X and Y sampleids do not match!&#34;)

        logging.log(
            f&#34;Passed sampleid check for neuroid {Y.neuroid.values}&#34;,
            verbosity_check=True,
        )

    def _drop_na(
        self, X: xr.DataArray, Y: xr.DataArray, dim: str = &#34;sampleid&#34;, **kwargs
    ):
        &#34;&#34;&#34;
        drop samples with missing values (based on Y) in X or Y along specified dimension
        Make sure that X and Y now have the same sampleids
        &#34;&#34;&#34;
        # limit data to current neuroid, and then drop the samples that are missing data for this neuroid
        Y_slice = Y.dropna(dim=dim, **kwargs)
        Y_filtered_ids = Y_slice[dim].values

        assert set(Y_filtered_ids).issubset(set(X[dim].values))

        logging.log(
            f&#34;for neuroid {Y_slice.neuroid.values}, we used {(num_retained := len(Y_filtered_ids))}&#34;
            f&#34; samples; dropped {len(Y[dim]) - num_retained}&#34;,
            verbosity_check=True,
        )

        # use only the samples that are in Y
        X_slice = X.sel(sampleid=Y_filtered_ids)

        return X_slice, Y_slice

    # def _permute_X(
    #     self,
    #     X: xr.DataArray,
    #     method: str = &#34;shuffle_X_rows&#34;,
    #     random_state: int = 42,
    # ):
    #     &#34;&#34;&#34;Permute the features of X.
    #
    #     Parameters
    #     ----------
    #     X : xr.DataArray
    #             The embeddings to be permuted
    #     method : str
    #             The method to use for permutation.
    #             &#39;shuffle_X_rows&#39; : Shuffle the rows of X (=shuffle the sentences and create a mismatch between the sentence embeddings and target)
    #             &#39;shuffle_each_X_col&#39;: For each column (=feature/unit) of X, permute that feature&#39;s values across all sentences.
    #                                                       Retains the statistics of the original features (e.g., mean per feature) but the values of the features are shuffled for each sentence.
    #     random_state : int
    #             The seed for the random number generator.
    #
    #     Returns
    #     -------
    #     xr.DataArray
    #             The permuted dataarray
    #     &#34;&#34;&#34;
    #
    #     X_orig = X.copy(deep=True)
    #
    #     if logging.get_verbosity():
    #         logging.log(f&#34;OBS: permuting X with method {method}&#34;)
    #
    #     if method == &#34;shuffle_X_rows&#34;:
    #         X = X.sample(
    #             n=X.shape[1], random_state=random_state
    #         )  # check whether X_shape is correct
    #
    #     elif method == &#34;shuffle_each_X_col&#34;:
    #         np.random.seed(random_state)
    #         for feat in X.data.shape[0]:  # per neuroid
    #             np.random.shuffle(X.data[feat, :])
    #
    #     else:
    #         raise ValueError(f&#34;Invalid method: {method}&#34;)
    #
    #     assert X.shape == X_orig.shape
    #     assert np.all(X.data != X_orig.data)
    #
    #     return X

    def fit_transform(
        self,
        X: xr.DataArray,
        Y: xr.DataArray,
        # permute_X: typing.Union[bool, str] = False,
        ceiling: bool = False,
        ceiling_coord: str = &#34;subject&#34;,
    ) -&gt; typing.Tuple[xr.DataArray, xr.DataArray]:
        &#34;&#34;&#34;creates a mapping model using k-fold cross-validation
            -&gt; uses params from the class initialization, uses strat_coord
               and split_coord to stratify and split across group boundaries

        Returns:
            [type]: [description]
        &#34;&#34;&#34;
        from sklearn.random_projection import GaussianRandomProjection

        if ceiling:
            n_neuroids = X.neuroid.values.size
            X = Y.copy()

        logging.log(f&#34;X shape: {X.data.shape}&#34;, verbosity_check=True)
        logging.log(f&#34;Y shape: {Y.data.shape}&#34;, verbosity_check=True)

        if self.strat_coord:
            try:
                assert (X[self.strat_coord].values == Y[self.strat_coord].values).all()
            except AssertionError as e:
                raise ValueError(
                    f&#34;{self.strat_coord} coordinate does not align across X and Y&#34;
                )
        if self.split_coord:
            try:
                assert (X[self.split_coord].values == Y[self.split_coord].values).all()
            except AssertionError as e:
                raise ValueError(
                    f&#34;{self.split_coord} coordinate does not align across X and Y&#34;
                )

        def fit_per_neuroid(neuroid):
            Y_neuroid = Y.sel(neuroid=neuroid)

            # limit data to current neuroid, and then drop the samples that are missing data for this neuroid
            X_slice, Y_slice = self._drop_na(X, Y_neuroid, dim=&#34;sampleid&#34;)

            # Assert that X and Y have the same sampleids
            self._check_sampleids(X_slice, Y_slice)

            # select relevant ceiling split
            if ceiling:
                X_slice = X_slice.isel(
                    neuroid=X_slice[ceiling_coord] != Y_slice[ceiling_coord]
                ).dropna(dim=&#34;neuroid&#34;)

            # We can perform various sanity checks by &#39;permuting&#39; the source, X
            # NOTE this is a test! do not use under normal workflow!
            # if permute_X:
            #     logging.log(
            #         f&#34;`permute_X` flag is enabled. only do this in an adversarial setting.&#34;,
            #         cmap=&#34;WARN&#34;,
            #         type=&#34;WARN&#34;,
            #         verbosity_check=True,
            #     )
            #     X_slice = self._permute_X(X_slice, method=permute_X)

            # these collections store each split for our records later
            # TODO we aren&#39;t saving this to the object instance yet
            train_indices = []
            test_indices = []
            # only used in case of ridge_cv or any duck type that uses an alpha hparam

            splits = self.construct_splits(Y_slice)

            # X_test_collection = []
            Y_test_collection = []
            Y_pred_collection = []

            for cvfoldid, (train_index, test_index) in enumerate(splits):

                train_indices.append(train_index)
                test_indices.append(test_index)

                # !! NOTE the _nan_removed variants instead of X and Y
                X_train, X_test = (
                    X_slice.sel(sampleid=Y_slice.sampleid.values[train_index]),
                    X_slice.sel(sampleid=Y_slice.sampleid.values[test_index]),
                )
                y_train, y_test = (
                    Y_slice.sel(sampleid=Y_slice.sampleid.values[train_index]),
                    Y_slice.sel(sampleid=Y_slice.sampleid.values[test_index]),
                )

                # empty list to house the y_predictions per timeid
                y_pred_over_time = []

                for timeid in y_train.timeid:

                    # TODO: change this code for models that also have a non-singleton timeid
                    # i.e., output evolves in time (RNN?)

                    x_model_train = X_train.sel(timeid=0).values
                    y_model_train = y_train.sel(timeid=timeid).values.reshape(-1, 1)

                    if ceiling and x_model_train.shape[1] &gt; n_neuroids:
                        projection = GaussianRandomProjection(
                            n_components=n_neuroids, random_state=0
                        )
                        x_model_train = projection.fit_transform(x_model_train)

                    self.model.fit(
                        x_model_train,
                        y_model_train,
                    )

                    # store the hparam values related to the fitted models
                    alpha = getattr(self.model, &#34;alpha_&#34;, np.nan)

                    # deepcopy `y_test` as `y_pred` to inherit some of the metadata and dims
                    # and then populate it with our new predicted values
                    y_pred = (
                        y_test.sel(timeid=timeid)
                        .copy(deep=True)
                        .expand_dims(&#34;timeid&#34;, 1)
                    )
                    x_model_test = X_test.sel(timeid=0)
                    if ceiling and x_model_train.shape[1] &gt; n_neuroids:
                        x_model_test = projection.transform(x_model_test)
                    y_pred.data = self.model.predict(x_model_test)  # y_pred
                    y_pred = y_pred.assign_coords(timeid=(&#34;timeid&#34;, [timeid]))
                    y_pred = y_pred.assign_coords(alpha=(&#34;timeid&#34;, [alpha]))
                    y_pred = y_pred.assign_coords(cvfoldid=(&#34;timeid&#34;, [cvfoldid]))
                    y_pred_over_time.append(y_pred)

                y_pred_over_time = xr.concat(y_pred_over_time, dim=&#34;timeid&#34;)
                Y_pred_collection.append(y_pred_over_time)
                Y_test_collection.append(y_test)

            Y_test = xr.concat(Y_test_collection, dim=&#34;sampleid&#34;).sortby(&#34;sampleid&#34;)
            Y_pred = xr.concat(Y_pred_collection, dim=&#34;sampleid&#34;).sortby(&#34;sampleid&#34;)

            # test.append(Y_test)
            # pred.append(Y_pred)
            return Y_test, Y_pred

        # Loop across each Y neuroid (target)
        test = []
        pred = []
        # TODO: parallelize using order-preserving joblib-mapping
        # for neuroid in tqdm(Y.neuroid.values, desc=&#34;fitting a model per neuroid&#34;):
        for t, p in Parallel(n_jobs=-2)(
            delayed(fit_per_neuroid)(neuroid)
            for neuroid in tqdm(Y.neuroid.values, desc=&#34;fitting a model per neuroid&#34;)
        ):
            test += [t]
            pred += [p]

        test_xr = xr.concat(test, dim=&#34;neuroid&#34;).transpose(
            &#34;sampleid&#34;, &#34;neuroid&#34;, &#34;timeid&#34;
        )
        pred_xr = xr.concat(pred, dim=&#34;neuroid&#34;).transpose(
            &#34;sampleid&#34;, &#34;neuroid&#34;, &#34;timeid&#34;
        )

        if test_xr.stimulus.ndim &gt; 1:
            test_xr = collapse_multidim_coord(test_xr, &#34;stimulus&#34;, &#34;sampleid&#34;)
        if pred_xr.stimulus.ndim &gt; 1:
            pred_xr = collapse_multidim_coord(pred_xr, &#34;stimulus&#34;, &#34;sampleid&#34;)

        return pred_xr, test_xr

    # def map(self, source, target) -&gt; None:
    #     &#39;&#39;&#39;
    #     the works: constructs splits, fits models for each split, then evaluates the fit
    #             of each split and returns the result (also for each split)
    #     &#39;&#39;&#39;
    #     pass

    def save_model(self) -&gt; None:
        &#34;&#34;&#34;TODO: stuff that needs to be saved eventually

        - model weights
        - CV stuff (if using CV); but all arguments needed for initializing, in general
            - n_splits
            - random_state
            - split indices (based on random seed)
            - params per split (alpha, lambda)
            - validation score, etc. for each CV split?
        &#34;&#34;&#34;
        pass

    def predict(self, source) -&gt; None:
        pass</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>langbrainscore.interface.mapping._Mapping</li>
<li>langbrainscore.interface.cacheable._Cacheable</li>
<li>typing.Protocol</li>
<li>typing.Generic</li>
<li>abc.ABC</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="langbrainscore.mapping.mapping.LearnedMap.construct_splits"><code class="name flex">
<span>def <span class="ident">construct_splits</span></span>(<span>self, A)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def construct_splits(self, A):
    return self._construct_splits(
        A,
        self.strat_coord,
        self.k_fold,
        self.split_coord,
        self.num_split_groups_out,
        random_seed=self.random_seed,
    )</code></pre>
</details>
</dd>
<dt id="langbrainscore.mapping.mapping.LearnedMap.fit_full"><code class="name flex">
<span>def <span class="ident">fit_full</span></span>(<span>self, X, Y)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fit_full(self, X, Y):
    # TODO
    self.fit(X, Y, k_folds=1)
    raise NotImplemented</code></pre>
</details>
</dd>
<dt id="langbrainscore.mapping.mapping.LearnedMap.fit_transform"><code class="name flex">
<span>def <span class="ident">fit_transform</span></span>(<span>self, X: xarray.core.dataarray.DataArray, Y: xarray.core.dataarray.DataArray, ceiling: bool = False, ceiling_coord: str = 'subject') ‑> Tuple[xarray.core.dataarray.DataArray, xarray.core.dataarray.DataArray]</span>
</code></dt>
<dd>
<div class="desc"><p>creates a mapping model using k-fold cross-validation
-&gt; uses params from the class initialization, uses strat_coord
and split_coord to stratify and split across group boundaries</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>[type]</code></dt>
<dd>[description]</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fit_transform(
    self,
    X: xr.DataArray,
    Y: xr.DataArray,
    # permute_X: typing.Union[bool, str] = False,
    ceiling: bool = False,
    ceiling_coord: str = &#34;subject&#34;,
) -&gt; typing.Tuple[xr.DataArray, xr.DataArray]:
    &#34;&#34;&#34;creates a mapping model using k-fold cross-validation
        -&gt; uses params from the class initialization, uses strat_coord
           and split_coord to stratify and split across group boundaries

    Returns:
        [type]: [description]
    &#34;&#34;&#34;
    from sklearn.random_projection import GaussianRandomProjection

    if ceiling:
        n_neuroids = X.neuroid.values.size
        X = Y.copy()

    logging.log(f&#34;X shape: {X.data.shape}&#34;, verbosity_check=True)
    logging.log(f&#34;Y shape: {Y.data.shape}&#34;, verbosity_check=True)

    if self.strat_coord:
        try:
            assert (X[self.strat_coord].values == Y[self.strat_coord].values).all()
        except AssertionError as e:
            raise ValueError(
                f&#34;{self.strat_coord} coordinate does not align across X and Y&#34;
            )
    if self.split_coord:
        try:
            assert (X[self.split_coord].values == Y[self.split_coord].values).all()
        except AssertionError as e:
            raise ValueError(
                f&#34;{self.split_coord} coordinate does not align across X and Y&#34;
            )

    def fit_per_neuroid(neuroid):
        Y_neuroid = Y.sel(neuroid=neuroid)

        # limit data to current neuroid, and then drop the samples that are missing data for this neuroid
        X_slice, Y_slice = self._drop_na(X, Y_neuroid, dim=&#34;sampleid&#34;)

        # Assert that X and Y have the same sampleids
        self._check_sampleids(X_slice, Y_slice)

        # select relevant ceiling split
        if ceiling:
            X_slice = X_slice.isel(
                neuroid=X_slice[ceiling_coord] != Y_slice[ceiling_coord]
            ).dropna(dim=&#34;neuroid&#34;)

        # We can perform various sanity checks by &#39;permuting&#39; the source, X
        # NOTE this is a test! do not use under normal workflow!
        # if permute_X:
        #     logging.log(
        #         f&#34;`permute_X` flag is enabled. only do this in an adversarial setting.&#34;,
        #         cmap=&#34;WARN&#34;,
        #         type=&#34;WARN&#34;,
        #         verbosity_check=True,
        #     )
        #     X_slice = self._permute_X(X_slice, method=permute_X)

        # these collections store each split for our records later
        # TODO we aren&#39;t saving this to the object instance yet
        train_indices = []
        test_indices = []
        # only used in case of ridge_cv or any duck type that uses an alpha hparam

        splits = self.construct_splits(Y_slice)

        # X_test_collection = []
        Y_test_collection = []
        Y_pred_collection = []

        for cvfoldid, (train_index, test_index) in enumerate(splits):

            train_indices.append(train_index)
            test_indices.append(test_index)

            # !! NOTE the _nan_removed variants instead of X and Y
            X_train, X_test = (
                X_slice.sel(sampleid=Y_slice.sampleid.values[train_index]),
                X_slice.sel(sampleid=Y_slice.sampleid.values[test_index]),
            )
            y_train, y_test = (
                Y_slice.sel(sampleid=Y_slice.sampleid.values[train_index]),
                Y_slice.sel(sampleid=Y_slice.sampleid.values[test_index]),
            )

            # empty list to house the y_predictions per timeid
            y_pred_over_time = []

            for timeid in y_train.timeid:

                # TODO: change this code for models that also have a non-singleton timeid
                # i.e., output evolves in time (RNN?)

                x_model_train = X_train.sel(timeid=0).values
                y_model_train = y_train.sel(timeid=timeid).values.reshape(-1, 1)

                if ceiling and x_model_train.shape[1] &gt; n_neuroids:
                    projection = GaussianRandomProjection(
                        n_components=n_neuroids, random_state=0
                    )
                    x_model_train = projection.fit_transform(x_model_train)

                self.model.fit(
                    x_model_train,
                    y_model_train,
                )

                # store the hparam values related to the fitted models
                alpha = getattr(self.model, &#34;alpha_&#34;, np.nan)

                # deepcopy `y_test` as `y_pred` to inherit some of the metadata and dims
                # and then populate it with our new predicted values
                y_pred = (
                    y_test.sel(timeid=timeid)
                    .copy(deep=True)
                    .expand_dims(&#34;timeid&#34;, 1)
                )
                x_model_test = X_test.sel(timeid=0)
                if ceiling and x_model_train.shape[1] &gt; n_neuroids:
                    x_model_test = projection.transform(x_model_test)
                y_pred.data = self.model.predict(x_model_test)  # y_pred
                y_pred = y_pred.assign_coords(timeid=(&#34;timeid&#34;, [timeid]))
                y_pred = y_pred.assign_coords(alpha=(&#34;timeid&#34;, [alpha]))
                y_pred = y_pred.assign_coords(cvfoldid=(&#34;timeid&#34;, [cvfoldid]))
                y_pred_over_time.append(y_pred)

            y_pred_over_time = xr.concat(y_pred_over_time, dim=&#34;timeid&#34;)
            Y_pred_collection.append(y_pred_over_time)
            Y_test_collection.append(y_test)

        Y_test = xr.concat(Y_test_collection, dim=&#34;sampleid&#34;).sortby(&#34;sampleid&#34;)
        Y_pred = xr.concat(Y_pred_collection, dim=&#34;sampleid&#34;).sortby(&#34;sampleid&#34;)

        # test.append(Y_test)
        # pred.append(Y_pred)
        return Y_test, Y_pred

    # Loop across each Y neuroid (target)
    test = []
    pred = []
    # TODO: parallelize using order-preserving joblib-mapping
    # for neuroid in tqdm(Y.neuroid.values, desc=&#34;fitting a model per neuroid&#34;):
    for t, p in Parallel(n_jobs=-2)(
        delayed(fit_per_neuroid)(neuroid)
        for neuroid in tqdm(Y.neuroid.values, desc=&#34;fitting a model per neuroid&#34;)
    ):
        test += [t]
        pred += [p]

    test_xr = xr.concat(test, dim=&#34;neuroid&#34;).transpose(
        &#34;sampleid&#34;, &#34;neuroid&#34;, &#34;timeid&#34;
    )
    pred_xr = xr.concat(pred, dim=&#34;neuroid&#34;).transpose(
        &#34;sampleid&#34;, &#34;neuroid&#34;, &#34;timeid&#34;
    )

    if test_xr.stimulus.ndim &gt; 1:
        test_xr = collapse_multidim_coord(test_xr, &#34;stimulus&#34;, &#34;sampleid&#34;)
    if pred_xr.stimulus.ndim &gt; 1:
        pred_xr = collapse_multidim_coord(pred_xr, &#34;stimulus&#34;, &#34;sampleid&#34;)

    return pred_xr, test_xr</code></pre>
</details>
</dd>
<dt id="langbrainscore.mapping.mapping.LearnedMap.predict"><code class="name flex">
<span>def <span class="ident">predict</span></span>(<span>self, source) ‑> None</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def predict(self, source) -&gt; None:
    pass</code></pre>
</details>
</dd>
<dt id="langbrainscore.mapping.mapping.LearnedMap.save_model"><code class="name flex">
<span>def <span class="ident">save_model</span></span>(<span>self) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>TODO: stuff that needs to be saved eventually</p>
<ul>
<li>model weights</li>
<li>CV stuff (if using CV); but all arguments needed for initializing, in general<ul>
<li>n_splits</li>
<li>random_state</li>
<li>split indices (based on random seed)</li>
<li>params per split (alpha, lambda)</li>
<li>validation score, etc. for each CV split?</li>
</ul>
</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def save_model(self) -&gt; None:
    &#34;&#34;&#34;TODO: stuff that needs to be saved eventually

    - model weights
    - CV stuff (if using CV); but all arguments needed for initializing, in general
        - n_splits
        - random_state
        - split indices (based on random seed)
        - params per split (alpha, lambda)
        - validation score, etc. for each CV split?
    &#34;&#34;&#34;
    pass</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="langbrainscore.mapping" href="index.html">langbrainscore.mapping</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="langbrainscore.mapping.mapping.IdentityMap" href="#langbrainscore.mapping.mapping.IdentityMap">IdentityMap</a></code></h4>
<ul class="">
<li><code><a title="langbrainscore.mapping.mapping.IdentityMap.fit_transform" href="#langbrainscore.mapping.mapping.IdentityMap.fit_transform">fit_transform</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="langbrainscore.mapping.mapping.LearnedMap" href="#langbrainscore.mapping.mapping.LearnedMap">LearnedMap</a></code></h4>
<ul class="">
<li><code><a title="langbrainscore.mapping.mapping.LearnedMap.construct_splits" href="#langbrainscore.mapping.mapping.LearnedMap.construct_splits">construct_splits</a></code></li>
<li><code><a title="langbrainscore.mapping.mapping.LearnedMap.fit_full" href="#langbrainscore.mapping.mapping.LearnedMap.fit_full">fit_full</a></code></li>
<li><code><a title="langbrainscore.mapping.mapping.LearnedMap.fit_transform" href="#langbrainscore.mapping.mapping.LearnedMap.fit_transform">fit_transform</a></code></li>
<li><code><a title="langbrainscore.mapping.mapping.LearnedMap.predict" href="#langbrainscore.mapping.mapping.LearnedMap.predict">predict</a></code></li>
<li><code><a title="langbrainscore.mapping.mapping.LearnedMap.save_model" href="#langbrainscore.mapping.mapping.LearnedMap.save_model">save_model</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>