<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>langbrainscore.dataset.dataset API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>langbrainscore.dataset.dataset</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python"># stdlib imports
import typing
from collections import abc
from pathlib import Path

import numpy as np
import xarray as xr
from joblib import Parallel, delayed
from langbrainscore.interface import _Dataset
from langbrainscore.utils.logging import log
from langbrainscore.utils.xarray import collapse_multidim_coord
from tqdm import tqdm


class Dataset(_Dataset):
    @property
    def contents(self) -&gt; xr.DataArray:
        &#34;&#34;&#34;
        access the internal xarray object. use with caution.
        &#34;&#34;&#34;
        return self._xr_obj

    @property
    def stimuli(self) -&gt; xr.DataArray:
        &#34;&#34;&#34;
        getter method that returns an xarray object of stimuli and associated metadata

        Returns:
            xr.DataArray: xarray object containing the stimuli from the dataset and associated metadata
        &#34;&#34;&#34;
        return self.contents.stimulus

    @property
    def dims(self) -&gt; tuple:
        &#34;&#34;&#34;
        getter method that returns internal xarray dimensions

        Returns:
            tuple[str]: dimensions of internal xarray object
        &#34;&#34;&#34;
        return self.contents.dims

    def to_netcdf(self, filename):
        &#34;&#34;&#34;
        outputs the xarray.DataArray object to a netCDF file identified by
        `filename`. if it already exists, overwrites it.
        &#34;&#34;&#34;
        if Path(filename).expanduser().resolve().exists():
            log(f&#34;{filename} already exists. overwriting.&#34;, type=&#34;WARN&#34;)
        self._xr_obj.to_netcdf(filename)

    @classmethod
    def load_netcdf(cls, filename):
        &#34;&#34;&#34;
        loads a netCDF object that contains a pre-packaged xarray instance from
        a file at `filename`.
        &#34;&#34;&#34;
        return cls(xr.load_dataarray(filename))

    @classmethod
    def from_file_or_url(
        cls,
        file_path_or_url: typing.Union[str, Path],
        data_column: str,
        sampleid_index: str,
        neuroid_index: str,
        stimuli_index: str,
        timeid_index: str = None,
        subject_index: str = None,
        sampleid_metadata: typing.Union[
            typing.Iterable[str], typing.Mapping[str, str]
        ] = None,
        neuroid_metadata: typing.Union[
            typing.Iterable[str], typing.Mapping[str, str]
        ] = None,
        timeid_metadata: typing.Union[
            typing.Iterable[str], typing.Mapping[str, str]
        ] = None,
        multidim_metadata: typing.Iterable[
            typing.Mapping[str, typing.Iterable[str]]
        ] = None,
        sort_by: typing.Iterable[str] = (),
        sep=&#34;,&#34;,
        parallel: int = -2,
    ) -&gt; _Dataset:
        &#34;&#34;&#34;Creates a Dataset object holding an `xr.DataArray` instance using a CSV file readable by pandas.
            Constructs the `xr.DataArray` using specified columns to construct dimensions and
            metadata along those dimensions in the form of coordinates.
            Minimally requires `sampleid` and `neuroid` to be provided.

            Note: Each row of the supplied file must have a single data point corresponding
            to a unique `sampleid`, `neuroid`, and `timeid` (unique dimension values).
            I.e., each neuroid (which could be a voxel, an ROI, a reaction time RT value, etc.)
            must be on a new line for the same stimulus trial at a certain time.
            If `timeid` and `subjectid` is not provided:
                - a singleton timeid dimension is created with the value &#34;0&#34; for each sample.
                - a singleton subjectid dimension is created with value &#34;0&#34; that spans the entire data.
            For help on what these terms mean, please visit the
            [xarray glossary page](https://xarray.pydata.org/en/stable/user-guide/terminology.html)


        Args:
            file_path_or_url (typing.Union[str, Path]): a path or URL to a csv file
            data_column (str): title of the column that holds the datapoints per unit of measurement
                (e.g., BOLD contrast effect size, reaction time, voltage amplitude, etc)
            sampleid_index (str): title of the column that should be used to construct an index for sampleids.
                this should be unique for each stimulus in the dataset.
            neuroid_index (str): title of the column that should be used to construct an index for neuroids.
                this should be unique for each point of measurement within a subject. e.g., voxel1, voxel2, ...
                neuroids in the packaged dataset are transformed to be a product of subject_index and neuroid_index.
            stimuli_index (str): title of the column that holds stimuli shown to participants
            timeid_index (str, optional): title of the column that holds timepoints of stimulus presentation.
                optional. if not provided, a singleton timepoint &#39;0&#39; is assigned to each datapoint. Defaults to None.
            subject_index (str, optional): title of the column specifiying subject IDs. Defaults to None.
            sampleid_metadata (typing.Union[typing.Iterable[str], typing.Mapping[str,str]], optional):
                names of columns (and optionally mapping of existing column names to new coordinate names)
                that supply metadata along the sampleid dimension. Defaults to None.
            neuroid_metadata (typing.Union[typing.Iterable[str], typing.Mapping[str,str]], optional):
                see `sampleid_metadata`. Defaults to None.
            timeid_metadata (typing.Union[typing.Iterable[str], typing.Mapping[str,str]], optional):
                see `sampleid_metadata`. Defaults to None.
            multidim_metadata (typing.Iterable[typing.Mapping[str, typing.Iterable[str]]], optional):
                metadata to go with more than one dimension. e.g., chunks of a stimulus that unfolds with time.
                currently `NotImplemented`. Defaults to None.
            sort_by (typing.Iterable[str], optional): Sort data by these columns while repackaging it.
                data is sorted by `sampleid_index`, `neuroid_index`, and `timeid_index` in addition to this
                value. Defaults to ().
            sep (str, optional): separator to read a value-delimited file. this argument is passed to pandas.
                Defaults to &#39;,&#39;.

        Raises:
            ValueError: _description_

        Returns:
            _Dataset: a subclass of the `_Dataset` interface with the packaged xarray.DataArray as a member.
        &#34;&#34;&#34;

        T = typing.TypeVar(&#34;T&#34;)

        def collapse_same_value(arr: typing.Iterable[T]) -&gt; T:
            &#34;&#34;&#34;
            makes sure each element in an iterable is identical (using __eq__)
            to every other element by value and returns (any) one of the elements.
            if a non-identical element (!=) is found, raises ValueError
            &#34;&#34;&#34;
            try:
                first_thing = next(iter(arr))
            except StopIteration:
                log(f&#34;failed to obtain value from {arr}&#34;, verbosity_check=True)
                return np.nan
            for each_thing in arr:
                if first_thing != each_thing:
                    raise ValueError(f&#34;{first_thing} != {each_thing}&#34;)
            return first_thing

        import pandas as pd

        if str(file_path_or_url).endswith(&#34;.parquet.gzip&#34;):
            try:
                df = pd.read_parquet(file_path_or_url)
            except Exception as invalid_file:
                raise ValueError(&#34;invalid parquet file / filename&#34;) from invalid_file
        else:
            try:
                df = pd.read_csv(file_path_or_url, sep=sep)
            except Exception as invalid_file:
                raise ValueError(&#34;invalid csv file / filename&#34;) from invalid_file

        if timeid_index is None:
            timeid_index = &#34;timeid&#34;
            # create singleton timeid
            # we don&#39;t need to inflate data since each datapoint will just
            # correspond to timeid == 0 per sample
            timeid_column = [0] * len(df)
            df[timeid_index] = timeid_column
        if subject_index is None:
            subject_index = &#34;subject&#34;
            # create singleton subjectID
            # we don&#39;t need to inflate data since each datapoint will just
            # correspond to subject == 0 per sample
            subject_column = [&#34;subject0&#34;] * len(df)
            df[subject_index] = subject_column
        if not parallel:
            parallel = 1

        subjects = list(set(df[subject_index]))
        sampleids = list(
            set(df[sampleid_index])
        )  # what happens when the same stimulus is shown multiple times?
        # it will add entries with same sampleid that will have to
        # then be differentiated on the basis of metadata only
        # https://i.imgur.com/4V2DsIo.png
        neuroids = list(set(df[neuroid_index]))
        timeids = list(set(df[timeid_index]))

        if not isinstance(sampleid_metadata, abc.Mapping):
            sampleid_metadata = {k: k for k in sampleid_metadata}
        if not isinstance(neuroid_metadata, abc.Mapping):
            neuroid_metadata = {k: k for k in neuroid_metadata}
        if not isinstance(timeid_metadata, abc.Mapping):
            timeid_metadata = {k: k for k in timeid_metadata or ()}

        df = df.sort_values(
            [*sort_by, subject_index, sampleid_index, neuroid_index, timeid_index]
        )

        def get_sampleid_xr(sampleid):
            sample_view = df[df[sampleid_index] == sampleid]  # why not sampleid_view?

            neuroid_xrs = []
            for neuroid in neuroids:
                neuroid_view = sample_view[sample_view[neuroid_index] == neuroid]

                timeid_xrs = []
                for timeid in timeids:
                    timeid_view = neuroid_view[neuroid_view[timeid_index] == timeid]
                    data = timeid_view[data_column].values
                    timeid_xr = xr.DataArray(
                        data.reshape(1, len(timeid_view[subject_index]), 1),
                        dims=(&#34;sampleid&#34;, &#34;neuroid&#34;, &#34;timeid&#34;),
                        coords={
                            &#34;sampleid&#34;: np.repeat(sampleid, 1),
                            &#34;neuroid&#34;: [
                                f&#34;{a}_{b}&#34;
                                for a, b in zip(
                                    timeid_view[subject_index],
                                    timeid_view[neuroid_index],
                                )
                            ],
                            &#34;timeid&#34;: np.repeat(timeid, 1),
                            &#34;subject&#34;: (&#34;neuroid&#34;, timeid_view[subject_index]),
                            &#34;stimulus&#34;: (
                                &#34;sampleid&#34;,
                                [collapse_same_value(timeid_view[stimuli_index])],
                            ),
                            **{
                                metadata_names[column]: (
                                    dimension,
                                    [collapse_same_value(timeid_view[column])],
                                )
                                for dimension, metadata_names in (
                                    (&#34;sampleid&#34;, sampleid_metadata),
                                    (&#34;timeid&#34;, timeid_metadata),
                                )
                                for column in metadata_names
                            },
                            **{
                                neuroid_metadata[column]: (
                                    &#34;neuroid&#34;,
                                    (timeid_view[column]),
                                )
                                for column in neuroid_metadata
                            },
                        },
                    )
                    timeid_xrs += [timeid_xr]

                neuroid_xr = xr.concat(timeid_xrs, dim=&#34;timeid&#34;)
                neuroid_xrs += [neuroid_xr]

            sampleid_xr = xr.concat(neuroid_xrs, dim=&#34;neuroid&#34;)
            return sampleid_xr

        sampleid_xrs = Parallel(n_jobs=parallel)(
            delayed(get_sampleid_xr)(sampleid)
            for sampleid in tqdm(sampleids, desc=&#34;reassembling data per sampleid&#34;)
        )

        unified_xr = xr.concat(sampleid_xrs, dim=&#34;sampleid&#34;)

        for dimension, metadata_names in (
            (&#34;sampleid&#34;, {**sampleid_metadata, &#34;stimulus&#34;: &#34;stimulus&#34;}),
            (&#34;timeid&#34;, timeid_metadata),
            (&#34;neuroid&#34;, {**neuroid_metadata, &#34;subject&#34;: &#34;subject&#34;}),
        ):
            for column in metadata_names:
                try:
                    unified_xr = collapse_multidim_coord(
                        unified_xr, metadata_names[column], dimension
                    )
                except ValueError as e:
                    log(
                        f&#34;dimension:{dimension}, column:{column}, shape:{unified_xr[metadata_names[column]].shape}&#34;,
                        type=&#34;ERR&#34;,
                    )

        return cls(unified_xr)  # NOTE: we use `cls` rather than `Dataset` so any
        # subclasses will use the subclass rather than parent</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="langbrainscore.dataset.dataset.Dataset"><code class="flex name class">
<span>class <span class="ident">Dataset</span></span>
<span>(</span><span>xr_obj: xarray.core.dataarray.DataArray, dataset_name: str = None)</span>
</code></dt>
<dd>
<div class="desc"><p>wrapper class for xarray DataArray that confirms format adheres to interface.</p>
<p>accepts an xarray with the following core
dimensions: sampleid, neuroid, timeid
and at least the following core
coordinates: sampleid, neuroid, timeid, stimulus, subject</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>xr_obj</code></strong> :&ensp;<code>xr.DataArray</code></dt>
<dd>xarray object with core dimensions and coordinates</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Dataset(_Dataset):
    @property
    def contents(self) -&gt; xr.DataArray:
        &#34;&#34;&#34;
        access the internal xarray object. use with caution.
        &#34;&#34;&#34;
        return self._xr_obj

    @property
    def stimuli(self) -&gt; xr.DataArray:
        &#34;&#34;&#34;
        getter method that returns an xarray object of stimuli and associated metadata

        Returns:
            xr.DataArray: xarray object containing the stimuli from the dataset and associated metadata
        &#34;&#34;&#34;
        return self.contents.stimulus

    @property
    def dims(self) -&gt; tuple:
        &#34;&#34;&#34;
        getter method that returns internal xarray dimensions

        Returns:
            tuple[str]: dimensions of internal xarray object
        &#34;&#34;&#34;
        return self.contents.dims

    def to_netcdf(self, filename):
        &#34;&#34;&#34;
        outputs the xarray.DataArray object to a netCDF file identified by
        `filename`. if it already exists, overwrites it.
        &#34;&#34;&#34;
        if Path(filename).expanduser().resolve().exists():
            log(f&#34;{filename} already exists. overwriting.&#34;, type=&#34;WARN&#34;)
        self._xr_obj.to_netcdf(filename)

    @classmethod
    def load_netcdf(cls, filename):
        &#34;&#34;&#34;
        loads a netCDF object that contains a pre-packaged xarray instance from
        a file at `filename`.
        &#34;&#34;&#34;
        return cls(xr.load_dataarray(filename))

    @classmethod
    def from_file_or_url(
        cls,
        file_path_or_url: typing.Union[str, Path],
        data_column: str,
        sampleid_index: str,
        neuroid_index: str,
        stimuli_index: str,
        timeid_index: str = None,
        subject_index: str = None,
        sampleid_metadata: typing.Union[
            typing.Iterable[str], typing.Mapping[str, str]
        ] = None,
        neuroid_metadata: typing.Union[
            typing.Iterable[str], typing.Mapping[str, str]
        ] = None,
        timeid_metadata: typing.Union[
            typing.Iterable[str], typing.Mapping[str, str]
        ] = None,
        multidim_metadata: typing.Iterable[
            typing.Mapping[str, typing.Iterable[str]]
        ] = None,
        sort_by: typing.Iterable[str] = (),
        sep=&#34;,&#34;,
        parallel: int = -2,
    ) -&gt; _Dataset:
        &#34;&#34;&#34;Creates a Dataset object holding an `xr.DataArray` instance using a CSV file readable by pandas.
            Constructs the `xr.DataArray` using specified columns to construct dimensions and
            metadata along those dimensions in the form of coordinates.
            Minimally requires `sampleid` and `neuroid` to be provided.

            Note: Each row of the supplied file must have a single data point corresponding
            to a unique `sampleid`, `neuroid`, and `timeid` (unique dimension values).
            I.e., each neuroid (which could be a voxel, an ROI, a reaction time RT value, etc.)
            must be on a new line for the same stimulus trial at a certain time.
            If `timeid` and `subjectid` is not provided:
                - a singleton timeid dimension is created with the value &#34;0&#34; for each sample.
                - a singleton subjectid dimension is created with value &#34;0&#34; that spans the entire data.
            For help on what these terms mean, please visit the
            [xarray glossary page](https://xarray.pydata.org/en/stable/user-guide/terminology.html)


        Args:
            file_path_or_url (typing.Union[str, Path]): a path or URL to a csv file
            data_column (str): title of the column that holds the datapoints per unit of measurement
                (e.g., BOLD contrast effect size, reaction time, voltage amplitude, etc)
            sampleid_index (str): title of the column that should be used to construct an index for sampleids.
                this should be unique for each stimulus in the dataset.
            neuroid_index (str): title of the column that should be used to construct an index for neuroids.
                this should be unique for each point of measurement within a subject. e.g., voxel1, voxel2, ...
                neuroids in the packaged dataset are transformed to be a product of subject_index and neuroid_index.
            stimuli_index (str): title of the column that holds stimuli shown to participants
            timeid_index (str, optional): title of the column that holds timepoints of stimulus presentation.
                optional. if not provided, a singleton timepoint &#39;0&#39; is assigned to each datapoint. Defaults to None.
            subject_index (str, optional): title of the column specifiying subject IDs. Defaults to None.
            sampleid_metadata (typing.Union[typing.Iterable[str], typing.Mapping[str,str]], optional):
                names of columns (and optionally mapping of existing column names to new coordinate names)
                that supply metadata along the sampleid dimension. Defaults to None.
            neuroid_metadata (typing.Union[typing.Iterable[str], typing.Mapping[str,str]], optional):
                see `sampleid_metadata`. Defaults to None.
            timeid_metadata (typing.Union[typing.Iterable[str], typing.Mapping[str,str]], optional):
                see `sampleid_metadata`. Defaults to None.
            multidim_metadata (typing.Iterable[typing.Mapping[str, typing.Iterable[str]]], optional):
                metadata to go with more than one dimension. e.g., chunks of a stimulus that unfolds with time.
                currently `NotImplemented`. Defaults to None.
            sort_by (typing.Iterable[str], optional): Sort data by these columns while repackaging it.
                data is sorted by `sampleid_index`, `neuroid_index`, and `timeid_index` in addition to this
                value. Defaults to ().
            sep (str, optional): separator to read a value-delimited file. this argument is passed to pandas.
                Defaults to &#39;,&#39;.

        Raises:
            ValueError: _description_

        Returns:
            _Dataset: a subclass of the `_Dataset` interface with the packaged xarray.DataArray as a member.
        &#34;&#34;&#34;

        T = typing.TypeVar(&#34;T&#34;)

        def collapse_same_value(arr: typing.Iterable[T]) -&gt; T:
            &#34;&#34;&#34;
            makes sure each element in an iterable is identical (using __eq__)
            to every other element by value and returns (any) one of the elements.
            if a non-identical element (!=) is found, raises ValueError
            &#34;&#34;&#34;
            try:
                first_thing = next(iter(arr))
            except StopIteration:
                log(f&#34;failed to obtain value from {arr}&#34;, verbosity_check=True)
                return np.nan
            for each_thing in arr:
                if first_thing != each_thing:
                    raise ValueError(f&#34;{first_thing} != {each_thing}&#34;)
            return first_thing

        import pandas as pd

        if str(file_path_or_url).endswith(&#34;.parquet.gzip&#34;):
            try:
                df = pd.read_parquet(file_path_or_url)
            except Exception as invalid_file:
                raise ValueError(&#34;invalid parquet file / filename&#34;) from invalid_file
        else:
            try:
                df = pd.read_csv(file_path_or_url, sep=sep)
            except Exception as invalid_file:
                raise ValueError(&#34;invalid csv file / filename&#34;) from invalid_file

        if timeid_index is None:
            timeid_index = &#34;timeid&#34;
            # create singleton timeid
            # we don&#39;t need to inflate data since each datapoint will just
            # correspond to timeid == 0 per sample
            timeid_column = [0] * len(df)
            df[timeid_index] = timeid_column
        if subject_index is None:
            subject_index = &#34;subject&#34;
            # create singleton subjectID
            # we don&#39;t need to inflate data since each datapoint will just
            # correspond to subject == 0 per sample
            subject_column = [&#34;subject0&#34;] * len(df)
            df[subject_index] = subject_column
        if not parallel:
            parallel = 1

        subjects = list(set(df[subject_index]))
        sampleids = list(
            set(df[sampleid_index])
        )  # what happens when the same stimulus is shown multiple times?
        # it will add entries with same sampleid that will have to
        # then be differentiated on the basis of metadata only
        # https://i.imgur.com/4V2DsIo.png
        neuroids = list(set(df[neuroid_index]))
        timeids = list(set(df[timeid_index]))

        if not isinstance(sampleid_metadata, abc.Mapping):
            sampleid_metadata = {k: k for k in sampleid_metadata}
        if not isinstance(neuroid_metadata, abc.Mapping):
            neuroid_metadata = {k: k for k in neuroid_metadata}
        if not isinstance(timeid_metadata, abc.Mapping):
            timeid_metadata = {k: k for k in timeid_metadata or ()}

        df = df.sort_values(
            [*sort_by, subject_index, sampleid_index, neuroid_index, timeid_index]
        )

        def get_sampleid_xr(sampleid):
            sample_view = df[df[sampleid_index] == sampleid]  # why not sampleid_view?

            neuroid_xrs = []
            for neuroid in neuroids:
                neuroid_view = sample_view[sample_view[neuroid_index] == neuroid]

                timeid_xrs = []
                for timeid in timeids:
                    timeid_view = neuroid_view[neuroid_view[timeid_index] == timeid]
                    data = timeid_view[data_column].values
                    timeid_xr = xr.DataArray(
                        data.reshape(1, len(timeid_view[subject_index]), 1),
                        dims=(&#34;sampleid&#34;, &#34;neuroid&#34;, &#34;timeid&#34;),
                        coords={
                            &#34;sampleid&#34;: np.repeat(sampleid, 1),
                            &#34;neuroid&#34;: [
                                f&#34;{a}_{b}&#34;
                                for a, b in zip(
                                    timeid_view[subject_index],
                                    timeid_view[neuroid_index],
                                )
                            ],
                            &#34;timeid&#34;: np.repeat(timeid, 1),
                            &#34;subject&#34;: (&#34;neuroid&#34;, timeid_view[subject_index]),
                            &#34;stimulus&#34;: (
                                &#34;sampleid&#34;,
                                [collapse_same_value(timeid_view[stimuli_index])],
                            ),
                            **{
                                metadata_names[column]: (
                                    dimension,
                                    [collapse_same_value(timeid_view[column])],
                                )
                                for dimension, metadata_names in (
                                    (&#34;sampleid&#34;, sampleid_metadata),
                                    (&#34;timeid&#34;, timeid_metadata),
                                )
                                for column in metadata_names
                            },
                            **{
                                neuroid_metadata[column]: (
                                    &#34;neuroid&#34;,
                                    (timeid_view[column]),
                                )
                                for column in neuroid_metadata
                            },
                        },
                    )
                    timeid_xrs += [timeid_xr]

                neuroid_xr = xr.concat(timeid_xrs, dim=&#34;timeid&#34;)
                neuroid_xrs += [neuroid_xr]

            sampleid_xr = xr.concat(neuroid_xrs, dim=&#34;neuroid&#34;)
            return sampleid_xr

        sampleid_xrs = Parallel(n_jobs=parallel)(
            delayed(get_sampleid_xr)(sampleid)
            for sampleid in tqdm(sampleids, desc=&#34;reassembling data per sampleid&#34;)
        )

        unified_xr = xr.concat(sampleid_xrs, dim=&#34;sampleid&#34;)

        for dimension, metadata_names in (
            (&#34;sampleid&#34;, {**sampleid_metadata, &#34;stimulus&#34;: &#34;stimulus&#34;}),
            (&#34;timeid&#34;, timeid_metadata),
            (&#34;neuroid&#34;, {**neuroid_metadata, &#34;subject&#34;: &#34;subject&#34;}),
        ):
            for column in metadata_names:
                try:
                    unified_xr = collapse_multidim_coord(
                        unified_xr, metadata_names[column], dimension
                    )
                except ValueError as e:
                    log(
                        f&#34;dimension:{dimension}, column:{column}, shape:{unified_xr[metadata_names[column]].shape}&#34;,
                        type=&#34;ERR&#34;,
                    )

        return cls(unified_xr)  # NOTE: we use `cls` rather than `Dataset` so any
        # subclasses will use the subclass rather than parent</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>langbrainscore.interface.dataset._Dataset</li>
<li>langbrainscore.interface.cacheable._Cacheable</li>
<li>typing.Protocol</li>
<li>typing.Generic</li>
<li>abc.ABC</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="langbrainscore.dataset.dataset.Dataset.dataset_name"><code class="name">var <span class="ident">dataset_name</span> : str</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Static methods</h3>
<dl>
<dt id="langbrainscore.dataset.dataset.Dataset.from_file_or_url"><code class="name flex">
<span>def <span class="ident">from_file_or_url</span></span>(<span>file_path_or_url: Union[str, pathlib.Path], data_column: str, sampleid_index: str, neuroid_index: str, stimuli_index: str, timeid_index: str = None, subject_index: str = None, sampleid_metadata: Union[Iterable[str], Mapping[str, str]] = None, neuroid_metadata: Union[Iterable[str], Mapping[str, str]] = None, timeid_metadata: Union[Iterable[str], Mapping[str, str]] = None, multidim_metadata: Iterable[Mapping[str, Iterable[str]]] = None, sort_by: Iterable[str] = (), sep=',', parallel: int = -2) ‑> langbrainscore.interface.dataset._Dataset</span>
</code></dt>
<dd>
<div class="desc"><p>Creates a Dataset object holding an <code>xr.DataArray</code> instance using a CSV file readable by pandas.
Constructs the <code>xr.DataArray</code> using specified columns to construct dimensions and
metadata along those dimensions in the form of coordinates.
Minimally requires <code>sampleid</code> and <code>neuroid</code> to be provided.</p>
<pre><code>Note: Each row of the supplied file must have a single data point corresponding
to a unique &lt;code&gt;sampleid&lt;/code&gt;, &lt;code&gt;neuroid&lt;/code&gt;, and &lt;code&gt;timeid&lt;/code&gt; (unique dimension values).
I.e., each neuroid (which could be a voxel, an ROI, a reaction time RT value, etc.)
must be on a new line for the same stimulus trial at a certain time.
If &lt;code&gt;timeid&lt;/code&gt; and &lt;code&gt;subjectid&lt;/code&gt; is not provided:
    - a singleton timeid dimension is created with the value "0" for each sample.
    - a singleton subjectid dimension is created with value "0" that spans the entire data.
For help on what these terms mean, please visit the
[xarray glossary page](https://xarray.pydata.org/en/stable/user-guide/terminology.html)
</code></pre>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>file_path_or_url</code></strong> :&ensp;<code>typing.Union[str, Path]</code></dt>
<dd>a path or URL to a csv file</dd>
<dt><strong><code>data_column</code></strong> :&ensp;<code>str</code></dt>
<dd>title of the column that holds the datapoints per unit of measurement
(e.g., BOLD contrast effect size, reaction time, voltage amplitude, etc)</dd>
<dt><strong><code>sampleid_index</code></strong> :&ensp;<code>str</code></dt>
<dd>title of the column that should be used to construct an index for sampleids.
this should be unique for each stimulus in the dataset.</dd>
<dt><strong><code>neuroid_index</code></strong> :&ensp;<code>str</code></dt>
<dd>title of the column that should be used to construct an index for neuroids.
this should be unique for each point of measurement within a subject. e.g., voxel1, voxel2, &hellip;
neuroids in the packaged dataset are transformed to be a product of subject_index and neuroid_index.</dd>
<dt><strong><code>stimuli_index</code></strong> :&ensp;<code>str</code></dt>
<dd>title of the column that holds stimuli shown to participants</dd>
<dt><strong><code>timeid_index</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>title of the column that holds timepoints of stimulus presentation.
optional. if not provided, a singleton timepoint '0' is assigned to each datapoint. Defaults to None.</dd>
<dt><strong><code>subject_index</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>title of the column specifiying subject IDs. Defaults to None.</dd>
<dt>sampleid_metadata (typing.Union[typing.Iterable[str], typing.Mapping[str,str]], optional):</dt>
<dt>names of columns (and optionally mapping of existing column names to new coordinate names)</dt>
<dt>that supply metadata along the sampleid dimension. Defaults to None.</dt>
<dt>neuroid_metadata (typing.Union[typing.Iterable[str], typing.Mapping[str,str]], optional):</dt>
<dt>see <code>sampleid_metadata</code>. Defaults to None.</dt>
<dt>timeid_metadata (typing.Union[typing.Iterable[str], typing.Mapping[str,str]], optional):</dt>
<dt>see <code>sampleid_metadata</code>. Defaults to None.</dt>
<dt>multidim_metadata (typing.Iterable[typing.Mapping[str, typing.Iterable[str]]], optional):</dt>
<dt>metadata to go with more than one dimension. e.g., chunks of a stimulus that unfolds with time.</dt>
<dt>currently <code>NotImplemented</code>. Defaults to None.</dt>
<dt><strong><code>sort_by</code></strong> :&ensp;<code>typing.Iterable[str]</code>, optional</dt>
<dd>Sort data by these columns while repackaging it.
data is sorted by <code>sampleid_index</code>, <code>neuroid_index</code>, and <code>timeid_index</code> in addition to this
value. Defaults to ().</dd>
<dt><strong><code>sep</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>separator to read a value-delimited file. this argument is passed to pandas.
Defaults to ','.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>ValueError</code></dt>
<dd><em>description</em></dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>_Dataset</code></dt>
<dd>a subclass of the <code>_Dataset</code> interface with the packaged xarray.DataArray as a member.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def from_file_or_url(
    cls,
    file_path_or_url: typing.Union[str, Path],
    data_column: str,
    sampleid_index: str,
    neuroid_index: str,
    stimuli_index: str,
    timeid_index: str = None,
    subject_index: str = None,
    sampleid_metadata: typing.Union[
        typing.Iterable[str], typing.Mapping[str, str]
    ] = None,
    neuroid_metadata: typing.Union[
        typing.Iterable[str], typing.Mapping[str, str]
    ] = None,
    timeid_metadata: typing.Union[
        typing.Iterable[str], typing.Mapping[str, str]
    ] = None,
    multidim_metadata: typing.Iterable[
        typing.Mapping[str, typing.Iterable[str]]
    ] = None,
    sort_by: typing.Iterable[str] = (),
    sep=&#34;,&#34;,
    parallel: int = -2,
) -&gt; _Dataset:
    &#34;&#34;&#34;Creates a Dataset object holding an `xr.DataArray` instance using a CSV file readable by pandas.
        Constructs the `xr.DataArray` using specified columns to construct dimensions and
        metadata along those dimensions in the form of coordinates.
        Minimally requires `sampleid` and `neuroid` to be provided.

        Note: Each row of the supplied file must have a single data point corresponding
        to a unique `sampleid`, `neuroid`, and `timeid` (unique dimension values).
        I.e., each neuroid (which could be a voxel, an ROI, a reaction time RT value, etc.)
        must be on a new line for the same stimulus trial at a certain time.
        If `timeid` and `subjectid` is not provided:
            - a singleton timeid dimension is created with the value &#34;0&#34; for each sample.
            - a singleton subjectid dimension is created with value &#34;0&#34; that spans the entire data.
        For help on what these terms mean, please visit the
        [xarray glossary page](https://xarray.pydata.org/en/stable/user-guide/terminology.html)


    Args:
        file_path_or_url (typing.Union[str, Path]): a path or URL to a csv file
        data_column (str): title of the column that holds the datapoints per unit of measurement
            (e.g., BOLD contrast effect size, reaction time, voltage amplitude, etc)
        sampleid_index (str): title of the column that should be used to construct an index for sampleids.
            this should be unique for each stimulus in the dataset.
        neuroid_index (str): title of the column that should be used to construct an index for neuroids.
            this should be unique for each point of measurement within a subject. e.g., voxel1, voxel2, ...
            neuroids in the packaged dataset are transformed to be a product of subject_index and neuroid_index.
        stimuli_index (str): title of the column that holds stimuli shown to participants
        timeid_index (str, optional): title of the column that holds timepoints of stimulus presentation.
            optional. if not provided, a singleton timepoint &#39;0&#39; is assigned to each datapoint. Defaults to None.
        subject_index (str, optional): title of the column specifiying subject IDs. Defaults to None.
        sampleid_metadata (typing.Union[typing.Iterable[str], typing.Mapping[str,str]], optional):
            names of columns (and optionally mapping of existing column names to new coordinate names)
            that supply metadata along the sampleid dimension. Defaults to None.
        neuroid_metadata (typing.Union[typing.Iterable[str], typing.Mapping[str,str]], optional):
            see `sampleid_metadata`. Defaults to None.
        timeid_metadata (typing.Union[typing.Iterable[str], typing.Mapping[str,str]], optional):
            see `sampleid_metadata`. Defaults to None.
        multidim_metadata (typing.Iterable[typing.Mapping[str, typing.Iterable[str]]], optional):
            metadata to go with more than one dimension. e.g., chunks of a stimulus that unfolds with time.
            currently `NotImplemented`. Defaults to None.
        sort_by (typing.Iterable[str], optional): Sort data by these columns while repackaging it.
            data is sorted by `sampleid_index`, `neuroid_index`, and `timeid_index` in addition to this
            value. Defaults to ().
        sep (str, optional): separator to read a value-delimited file. this argument is passed to pandas.
            Defaults to &#39;,&#39;.

    Raises:
        ValueError: _description_

    Returns:
        _Dataset: a subclass of the `_Dataset` interface with the packaged xarray.DataArray as a member.
    &#34;&#34;&#34;

    T = typing.TypeVar(&#34;T&#34;)

    def collapse_same_value(arr: typing.Iterable[T]) -&gt; T:
        &#34;&#34;&#34;
        makes sure each element in an iterable is identical (using __eq__)
        to every other element by value and returns (any) one of the elements.
        if a non-identical element (!=) is found, raises ValueError
        &#34;&#34;&#34;
        try:
            first_thing = next(iter(arr))
        except StopIteration:
            log(f&#34;failed to obtain value from {arr}&#34;, verbosity_check=True)
            return np.nan
        for each_thing in arr:
            if first_thing != each_thing:
                raise ValueError(f&#34;{first_thing} != {each_thing}&#34;)
        return first_thing

    import pandas as pd

    if str(file_path_or_url).endswith(&#34;.parquet.gzip&#34;):
        try:
            df = pd.read_parquet(file_path_or_url)
        except Exception as invalid_file:
            raise ValueError(&#34;invalid parquet file / filename&#34;) from invalid_file
    else:
        try:
            df = pd.read_csv(file_path_or_url, sep=sep)
        except Exception as invalid_file:
            raise ValueError(&#34;invalid csv file / filename&#34;) from invalid_file

    if timeid_index is None:
        timeid_index = &#34;timeid&#34;
        # create singleton timeid
        # we don&#39;t need to inflate data since each datapoint will just
        # correspond to timeid == 0 per sample
        timeid_column = [0] * len(df)
        df[timeid_index] = timeid_column
    if subject_index is None:
        subject_index = &#34;subject&#34;
        # create singleton subjectID
        # we don&#39;t need to inflate data since each datapoint will just
        # correspond to subject == 0 per sample
        subject_column = [&#34;subject0&#34;] * len(df)
        df[subject_index] = subject_column
    if not parallel:
        parallel = 1

    subjects = list(set(df[subject_index]))
    sampleids = list(
        set(df[sampleid_index])
    )  # what happens when the same stimulus is shown multiple times?
    # it will add entries with same sampleid that will have to
    # then be differentiated on the basis of metadata only
    # https://i.imgur.com/4V2DsIo.png
    neuroids = list(set(df[neuroid_index]))
    timeids = list(set(df[timeid_index]))

    if not isinstance(sampleid_metadata, abc.Mapping):
        sampleid_metadata = {k: k for k in sampleid_metadata}
    if not isinstance(neuroid_metadata, abc.Mapping):
        neuroid_metadata = {k: k for k in neuroid_metadata}
    if not isinstance(timeid_metadata, abc.Mapping):
        timeid_metadata = {k: k for k in timeid_metadata or ()}

    df = df.sort_values(
        [*sort_by, subject_index, sampleid_index, neuroid_index, timeid_index]
    )

    def get_sampleid_xr(sampleid):
        sample_view = df[df[sampleid_index] == sampleid]  # why not sampleid_view?

        neuroid_xrs = []
        for neuroid in neuroids:
            neuroid_view = sample_view[sample_view[neuroid_index] == neuroid]

            timeid_xrs = []
            for timeid in timeids:
                timeid_view = neuroid_view[neuroid_view[timeid_index] == timeid]
                data = timeid_view[data_column].values
                timeid_xr = xr.DataArray(
                    data.reshape(1, len(timeid_view[subject_index]), 1),
                    dims=(&#34;sampleid&#34;, &#34;neuroid&#34;, &#34;timeid&#34;),
                    coords={
                        &#34;sampleid&#34;: np.repeat(sampleid, 1),
                        &#34;neuroid&#34;: [
                            f&#34;{a}_{b}&#34;
                            for a, b in zip(
                                timeid_view[subject_index],
                                timeid_view[neuroid_index],
                            )
                        ],
                        &#34;timeid&#34;: np.repeat(timeid, 1),
                        &#34;subject&#34;: (&#34;neuroid&#34;, timeid_view[subject_index]),
                        &#34;stimulus&#34;: (
                            &#34;sampleid&#34;,
                            [collapse_same_value(timeid_view[stimuli_index])],
                        ),
                        **{
                            metadata_names[column]: (
                                dimension,
                                [collapse_same_value(timeid_view[column])],
                            )
                            for dimension, metadata_names in (
                                (&#34;sampleid&#34;, sampleid_metadata),
                                (&#34;timeid&#34;, timeid_metadata),
                            )
                            for column in metadata_names
                        },
                        **{
                            neuroid_metadata[column]: (
                                &#34;neuroid&#34;,
                                (timeid_view[column]),
                            )
                            for column in neuroid_metadata
                        },
                    },
                )
                timeid_xrs += [timeid_xr]

            neuroid_xr = xr.concat(timeid_xrs, dim=&#34;timeid&#34;)
            neuroid_xrs += [neuroid_xr]

        sampleid_xr = xr.concat(neuroid_xrs, dim=&#34;neuroid&#34;)
        return sampleid_xr

    sampleid_xrs = Parallel(n_jobs=parallel)(
        delayed(get_sampleid_xr)(sampleid)
        for sampleid in tqdm(sampleids, desc=&#34;reassembling data per sampleid&#34;)
    )

    unified_xr = xr.concat(sampleid_xrs, dim=&#34;sampleid&#34;)

    for dimension, metadata_names in (
        (&#34;sampleid&#34;, {**sampleid_metadata, &#34;stimulus&#34;: &#34;stimulus&#34;}),
        (&#34;timeid&#34;, timeid_metadata),
        (&#34;neuroid&#34;, {**neuroid_metadata, &#34;subject&#34;: &#34;subject&#34;}),
    ):
        for column in metadata_names:
            try:
                unified_xr = collapse_multidim_coord(
                    unified_xr, metadata_names[column], dimension
                )
            except ValueError as e:
                log(
                    f&#34;dimension:{dimension}, column:{column}, shape:{unified_xr[metadata_names[column]].shape}&#34;,
                    type=&#34;ERR&#34;,
                )

    return cls(unified_xr)  # NOTE: we use `cls` rather than `Dataset` so any
    # subclasses will use the subclass rather than parent</code></pre>
</details>
</dd>
<dt id="langbrainscore.dataset.dataset.Dataset.load_netcdf"><code class="name flex">
<span>def <span class="ident">load_netcdf</span></span>(<span>filename)</span>
</code></dt>
<dd>
<div class="desc"><p>loads a netCDF object that contains a pre-packaged xarray instance from
a file at <code>filename</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def load_netcdf(cls, filename):
    &#34;&#34;&#34;
    loads a netCDF object that contains a pre-packaged xarray instance from
    a file at `filename`.
    &#34;&#34;&#34;
    return cls(xr.load_dataarray(filename))</code></pre>
</details>
</dd>
</dl>
<h3>Instance variables</h3>
<dl>
<dt id="langbrainscore.dataset.dataset.Dataset.contents"><code class="name">var <span class="ident">contents</span> : xarray.core.dataarray.DataArray</code></dt>
<dd>
<div class="desc"><p>access the internal xarray object. use with caution.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def contents(self) -&gt; xr.DataArray:
    &#34;&#34;&#34;
    access the internal xarray object. use with caution.
    &#34;&#34;&#34;
    return self._xr_obj</code></pre>
</details>
</dd>
<dt id="langbrainscore.dataset.dataset.Dataset.dims"><code class="name">var <span class="ident">dims</span> : tuple</code></dt>
<dd>
<div class="desc"><p>getter method that returns internal xarray dimensions</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>tuple[str]</code></dt>
<dd>dimensions of internal xarray object</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def dims(self) -&gt; tuple:
    &#34;&#34;&#34;
    getter method that returns internal xarray dimensions

    Returns:
        tuple[str]: dimensions of internal xarray object
    &#34;&#34;&#34;
    return self.contents.dims</code></pre>
</details>
</dd>
<dt id="langbrainscore.dataset.dataset.Dataset.stimuli"><code class="name">var <span class="ident">stimuli</span> : xarray.core.dataarray.DataArray</code></dt>
<dd>
<div class="desc"><p>getter method that returns an xarray object of stimuli and associated metadata</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>xr.DataArray</code></dt>
<dd>xarray object containing the stimuli from the dataset and associated metadata</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def stimuli(self) -&gt; xr.DataArray:
    &#34;&#34;&#34;
    getter method that returns an xarray object of stimuli and associated metadata

    Returns:
        xr.DataArray: xarray object containing the stimuli from the dataset and associated metadata
    &#34;&#34;&#34;
    return self.contents.stimulus</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="langbrainscore.dataset.dataset.Dataset.to_netcdf"><code class="name flex">
<span>def <span class="ident">to_netcdf</span></span>(<span>self, filename)</span>
</code></dt>
<dd>
<div class="desc"><p>outputs the xarray.DataArray object to a netCDF file identified by
<code>filename</code>. if it already exists, overwrites it.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to_netcdf(self, filename):
    &#34;&#34;&#34;
    outputs the xarray.DataArray object to a netCDF file identified by
    `filename`. if it already exists, overwrites it.
    &#34;&#34;&#34;
    if Path(filename).expanduser().resolve().exists():
        log(f&#34;{filename} already exists. overwriting.&#34;, type=&#34;WARN&#34;)
    self._xr_obj.to_netcdf(filename)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="langbrainscore.dataset" href="index.html">langbrainscore.dataset</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="langbrainscore.dataset.dataset.Dataset" href="#langbrainscore.dataset.dataset.Dataset">Dataset</a></code></h4>
<ul class="two-column">
<li><code><a title="langbrainscore.dataset.dataset.Dataset.contents" href="#langbrainscore.dataset.dataset.Dataset.contents">contents</a></code></li>
<li><code><a title="langbrainscore.dataset.dataset.Dataset.dataset_name" href="#langbrainscore.dataset.dataset.Dataset.dataset_name">dataset_name</a></code></li>
<li><code><a title="langbrainscore.dataset.dataset.Dataset.dims" href="#langbrainscore.dataset.dataset.Dataset.dims">dims</a></code></li>
<li><code><a title="langbrainscore.dataset.dataset.Dataset.from_file_or_url" href="#langbrainscore.dataset.dataset.Dataset.from_file_or_url">from_file_or_url</a></code></li>
<li><code><a title="langbrainscore.dataset.dataset.Dataset.load_netcdf" href="#langbrainscore.dataset.dataset.Dataset.load_netcdf">load_netcdf</a></code></li>
<li><code><a title="langbrainscore.dataset.dataset.Dataset.stimuli" href="#langbrainscore.dataset.dataset.Dataset.stimuli">stimuli</a></code></li>
<li><code><a title="langbrainscore.dataset.dataset.Dataset.to_netcdf" href="#langbrainscore.dataset.dataset.Dataset.to_netcdf">to_netcdf</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>